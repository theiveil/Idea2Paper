import re
from typing import Dict, List, Tuple, Optional

from idea2paper.config import PipelineConfig
from idea2paper.infra.llm import call_llm, parse_json_from_llm
from idea2paper.review.review_index import ReviewIndex
from idea2paper.infra.run_context import get_logger


def _sigmoid(x: float) -> float:
    return 1 / (1 + pow(2.718281828459045, -x))


def _safe_mean(values: List[float]) -> float:
    if not values:
        return 0.0
    return sum(values) / len(values)


class MultiAgentCritic:
    """å¤šæ™ºèƒ½ä½“è¯„å®¡å›¢: ä¸‰ä¸ªè§’è‰²è¯„å®¡ Story"""

    def __init__(self, review_index: Optional[ReviewIndex] = None):
        self.review_index = review_index
        self.reviewers = [
            {'name': 'Reviewer A', 'role': 'Methodology', 'focus': 'æŠ€æœ¯åˆç†æ€§'},
            {'name': 'Reviewer B', 'role': 'Novelty', 'focus': 'åˆ›æ–°æ€§'},
            {'name': 'Reviewer C', 'role': 'Storyteller', 'focus': 'å™äº‹å®Œæ•´æ€§'}
        ]

    def _log_event(self, event_type: str, payload: Dict):
        logger = get_logger()
        if logger:
            logger.log_event(event_type, payload)

    def _suspect_truncation(self, text: str) -> bool:
        if not text:
            return True
        stripped = text.strip()
        if not stripped:
            return True
        if stripped.count('{') > stripped.count('}'):
            return True
        if stripped.count('[') > stripped.count(']'):
            return True
        if not (stripped.endswith('}') or stripped.endswith(']') or stripped.endswith('```')):
            return True
        return False

    def _validate_comparisons(self, result: Dict, anchors: List[Dict]) -> Tuple[bool, str, Dict]:
        if not isinstance(result, dict):
            return False, "schema_invalid", {}
        comparisons = result.get("comparisons")
        if not isinstance(comparisons, list):
            return False, "schema_invalid", {}

        anchor_map = {a.get("paper_id"): a for a in anchors if a.get("paper_id")}
        if not anchor_map:
            return False, "missing_anchors", {}

        seen = set()
        normalized = []
        for comp in comparisons:
            if not isinstance(comp, dict):
                continue
            pid = comp.get("paper_id")
            if pid not in anchor_map or pid in seen:
                continue
            judgement = comp.get("judgement")
            confidence = comp.get("confidence")
            rationale = comp.get("rationale")
            if judgement not in ("better", "tie", "worse"):
                return False, "schema_invalid", {}
            try:
                confidence = float(confidence)
            except Exception:
                return False, "schema_invalid", {}
            if confidence < 0.0 or confidence > 1.0:
                return False, "schema_invalid", {}
            if not isinstance(rationale, str):
                return False, "schema_invalid", {}
            score_text = f"{anchor_map[pid]['score10']:.1f}"
            if "score10" not in rationale.lower() or score_text not in rationale:
                return False, "missing_score10", {}
            normalized.append({
                "paper_id": pid,
                "judgement": judgement,
                "confidence": confidence,
                "rationale": rationale
            })
            seen.add(pid)

        if len(seen) != len(anchor_map):
            return False, "missing_anchors", {}

        ordered = []
        for a in anchors:
            pid = a.get("paper_id")
            for comp in normalized:
                if comp["paper_id"] == pid:
                    ordered.append(comp)
                    break

        main_gaps = result.get("main_gaps", [])
        if not isinstance(main_gaps, list):
            main_gaps = []

        return True, "", {"comparisons": ordered, "main_gaps": main_gaps}

    def _build_anchor_prompt(self, story: Dict, reviewer: Dict, anchors: List[Dict]) -> str:
        problem_text = story.get('problem_framing') or story.get('problem_definition', '')
        method_text = story.get('method_skeleton', '')
        if isinstance(method_text, dict):
            method_text = ' '.join(str(v) for v in method_text.values() if v)

        anchor_lines = []
        for a in anchors:
            anchor_lines.append(
                f"- paper_id: {a['paper_id']} | title: {a.get('title','')} | score10: {a['score10']:.1f}"
            )
        anchor_text = "\n".join(anchor_lines)

        return f"""
You are a strict reviewer ({reviewer['role']}) for top-tier ML/NLP conferences.
You must NOT output a direct score. Only compare the Story against anchor papers with real review scores.

Anchors (score10 comes from real review statistics):
{anchor_text}

Story:
Title: {story.get('title','')}
Abstract: {story.get('abstract','')}
Problem: {problem_text}
Method: {method_text}
Claims: {', '.join(story.get('innovation_claims', []))}
Experiments: {story.get('experiments_plan','')}

Task:
For each anchor, decide whether the Story is better, tie, or worse on {reviewer['role']}, and provide confidence (0-1).
You must mention the anchor's score10 in the rationale using the format "score10: X.X".
Comparisons must include every anchor exactly once.
Each rationale must be ONE sentence (<=25 words).

Output JSON ONLY. No markdown, no extra text:
{{
  "comparisons": [
    {{"paper_id":"...", "judgement":"better|tie|worse", "confidence":0.0-1.0, "rationale":"...score10: X.X..."}}
  ],
  "main_gaps": ["gap1", "gap2", "gap3"]
}}
"""

    def _build_repair_prompt(self, previous_text: str, anchors: List[Dict], reviewer_role: str) -> str:
        anchor_lines = []
        for a in anchors:
            anchor_lines.append(f"- paper_id: {a['paper_id']} | score10: {a['score10']:.1f}")
        anchor_text = "\n".join(anchor_lines)
        truncated_prev = previous_text[:6000]
        return f"""
You must fix the previous output into STRICT valid JSON.
Role: {reviewer_role}

Anchors (must cover ALL exactly once):
{anchor_text}

Rules:
1) Output JSON ONLY (no markdown, no explanations).
2) "comparisons" length MUST equal number of anchors.
3) Each comparison's paper_id must be from the anchor list.
4) judgement must be one of: better|tie|worse.
5) confidence must be a number in [0,1].
6) rationale must be ONE sentence and MUST include "score10: X.X" for that anchor.

Previous output to repair:
{truncated_prev}

Return ONLY the corrected JSON:
{{
  "comparisons": [
    {{"paper_id":"...", "judgement":"better|tie|worse", "confidence":0.0-1.0, "rationale":"...score10: X.X..."}}
  ],
  "main_gaps": ["gap1", "gap2", "gap3"]
}}
"""

    def _build_reemit_prompt(self, story: Dict, reviewer: Dict, anchors: List[Dict]) -> str:
        return self._build_anchor_prompt(story, reviewer, anchors)

    def _get_comparisons_with_retries(
        self,
        story: Dict,
        reviewer: Dict,
        anchors: List[Dict],
        pattern_id: str
    ) -> Tuple[List[Dict], List[str]]:
        base_prompt = self._build_anchor_prompt(story, reviewer, anchors)
        response = call_llm(base_prompt, temperature=0.0, max_tokens=800, timeout=180)
        result = parse_json_from_llm(response)
        ok, reason, normalized = (False, "parse_failed", {})
        if result:
            ok, reason, normalized = self._validate_comparisons(result, anchors)

        if ok:
            return normalized["comparisons"], normalized.get("main_gaps", [])

        self._log_event("critic_invalid_output", {
            "pattern_id": pattern_id,
            "role": reviewer.get("role"),
            "attempt": 0,
            "reason": reason,
            "response_len": len(response),
            "truncated_suspected": self._suspect_truncation(response)
        })

        retries = getattr(PipelineConfig, "CRITIC_JSON_RETRIES", 2)
        last_reason = reason
        last_response = response

        for attempt in range(1, retries + 1):
            truncated = self._suspect_truncation(last_response)
            strategy = "reemit" if truncated else "repair"
            if strategy == "repair":
                prompt = self._build_repair_prompt(last_response, anchors, reviewer.get("role", ""))
            else:
                prompt = self._build_reemit_prompt(story, reviewer, anchors)

            print(f"   â³ Critic JSON retry {attempt}/{retries} ({strategy})...")
            response = call_llm(prompt, temperature=0.0, max_tokens=800, timeout=180)
            result = parse_json_from_llm(response)
            ok, reason, normalized = (False, "parse_failed", {})
            if result:
                ok, reason, normalized = self._validate_comparisons(result, anchors)

            if ok:
                self._log_event("critic_parse_recovered", {
                    "pattern_id": pattern_id,
                    "role": reviewer.get("role"),
                    "attempt": attempt,
                    "strategy": strategy
                })
                return normalized["comparisons"], normalized.get("main_gaps", [])

            self._log_event("critic_invalid_output", {
                "pattern_id": pattern_id,
                "role": reviewer.get("role"),
                "attempt": attempt,
                "reason": reason,
                "response_len": len(response),
                "truncated_suspected": self._suspect_truncation(response),
                "strategy": strategy
            })
            last_reason = reason
            last_response = response

        if getattr(PipelineConfig, "CRITIC_STRICT_JSON", True):
            self._log_event("critic_invalid_output_fatal", {
                "pattern_id": pattern_id,
                "role": reviewer.get("role"),
                "attempts": retries,
                "reason": last_reason
            })
            raise RuntimeError(
                f"Critic JSON invalid after retries: role={reviewer.get('role')} "
                f"pattern_id={pattern_id}, reason={last_reason}"
            )

        self._log_event("critic_fallback_neutral", {
            "pattern_id": pattern_id,
            "role": reviewer.get("role"),
            "attempts": retries,
            "reason": last_reason
        })
        comparisons = [
            {"paper_id": a["paper_id"], "judgement": "tie", "confidence": 0.0, "rationale": "LLM output unavailable"}
            for a in anchors
        ]
        main_gaps = ["LLM output unavailable; fallback to neutral comparisons."]
        return comparisons, main_gaps

    def _compute_pass_decision(self, avg_score: float, role_scores: Dict[str, float], pattern_id: str) -> Tuple[bool, Dict]:
        mode = getattr(PipelineConfig, "PASS_MODE", "two_of_three_q75_and_avg_ge_q50")
        min_papers = getattr(PipelineConfig, "PASS_MIN_PATTERN_PAPERS", 20)
        fallback = getattr(PipelineConfig, "PASS_FALLBACK", "global")

        used_distribution = "fixed"
        pattern_stats_n = 0
        q50 = None
        q75 = None

        if self.review_index and pattern_id:
            stats = self.review_index.get_pattern_quantiles(pattern_id, [0.5, 0.75])
            pattern_stats_n = int(stats.get("n", 0) or 0)
            if pattern_stats_n >= min_papers and stats.get("q50") is not None and stats.get("q75") is not None:
                q50 = float(stats.get("q50"))
                q75 = float(stats.get("q75"))
                used_distribution = "pattern"
            else:
                if fallback == "global":
                    gstats = self.review_index.get_global_quantiles([0.5, 0.75])
                    if gstats.get("q50") is not None and gstats.get("q75") is not None:
                        q50 = float(gstats.get("q50"))
                        q75 = float(gstats.get("q75"))
                        used_distribution = "global"
                if used_distribution == "fixed":
                    q50 = None
                    q75 = None

        roles = ["Methodology", "Novelty", "Storyteller"]
        roles_ge_q75 = {r: False for r in roles}
        count_ge_q75 = 0
        avg_ge_q50 = None

        passed = False
        if mode == "two_of_three_q75_and_avg_ge_q50" and q50 is not None and q75 is not None:
            roles_ge_q75 = {r: float(role_scores.get(r, 0.0)) >= q75 for r in roles}
            count_ge_q75 = sum(1 for v in roles_ge_q75.values() if v)
            avg_ge_q50 = avg_score >= q50
            passed = (count_ge_q75 >= 2) and avg_ge_q50
        else:
            passed = avg_score >= PipelineConfig.PASS_SCORE

        pass_audit = {
            "mode": mode if mode else "fixed",
            "used_distribution": used_distribution,
            "pattern_paper_count": pattern_stats_n,
            "q50": q50,
            "q75": q75,
            "count_roles_ge_q75": count_ge_q75,
            "roles_ge_q75": roles_ge_q75,
            "avg_ge_q50": avg_ge_q50,
            "avg_score": avg_score
        }

        return passed, pass_audit
    def review(self, story: Dict, context: Optional[Dict] = None) -> Dict:
        """è¯„å®¡ Story

        Returns:
            {
                'pass': bool,
                'avg_score': float,
                'reviews': [
                    {'reviewer': str, 'role': str, 'score': float, 'feedback': str},
                    ...
                ],
                'main_issue': str,  # 'novelty' | 'stability' | 'domain_distance'
                'suggestions': List[str]
            }
        """
        print("\n" + "=" * 80)
        print("ğŸ” Phase 3: Multi-Agent Critic (å¤šæ™ºèƒ½ä½“è¯„å®¡ - Anchored)")
        print("=" * 80)

        context = context or {}
        pattern_id = context.get("pattern_id", "")
        pattern_info = context.get("pattern_info", {}) or {}
        anchors = context.get("anchors", []) or []

        if not anchors and pattern_id and self.review_index:
            anchors = self.review_index.select_initial_anchors(
                pattern_id,
                pattern_info,
                max_initial=getattr(PipelineConfig, "ANCHOR_MAX_INITIAL", 7),
                quantiles=getattr(PipelineConfig, "ANCHOR_QUANTILES", [0.1, 0.25, 0.5, 0.75, 0.9]),
                max_exemplars=getattr(PipelineConfig, "ANCHOR_MAX_EXEMPLARS", 2),
            )

        if not anchors:
            # Fallback: deterministic neutral scoring
            reviews = []
            scores = []
            for reviewer in self.reviewers:
                reviews.append({
                    'reviewer': reviewer['name'],
                    'role': reviewer['role'],
                    'score': 5.0,
                    'feedback': 'No anchors available; defaulted to neutral score.'
                })
                scores.append(5.0)
            avg_score = _safe_mean(scores)
            main_issue, suggestions = self._diagnose_issue(reviews, scores)
            return {
                'pass': avg_score >= PipelineConfig.PASS_SCORE,
                'avg_score': avg_score,
                'reviews': reviews,
                'main_issue': main_issue,
                'suggestions': suggestions,
                'audit': {
                    'pattern_id': pattern_id,
                    'anchors': [],
                    'role_details': {}
                }
            }

        # Round 1 anchored review
        round1 = self._anchored_reviews(story, anchors, pattern_id)
        densify_needed = any(
            (r['loss'] > getattr(PipelineConfig, "DENSIFY_LOSS_THRESHOLD", 0.03)) or
            (r['monotonic_violations'] >= 1) or
            (r['avg_confidence'] < getattr(PipelineConfig, "DENSIFY_MIN_AVG_CONF", 0.45))
            for r in round1['role_details'].values()
        )
        densify_enabled = getattr(PipelineConfig, "ANCHOR_DENSIFY_ENABLE", True)

        anchors_rounds = [anchors]
        if densify_needed and not densify_enabled:
            self._log_event("critic_densify_skipped", {
                "pattern_id": pattern_id,
                "reason": "disabled",
            })
        if densify_enabled and densify_needed and pattern_id and self.review_index:
            scores_round1 = [r['score'] for r in round1['role_details'].values()]
            S_hint = _safe_mean(scores_round1) if scores_round1 else 5.0
            additional = self.review_index.select_adaptive_anchors(
                pattern_id,
                selected_ids=[a["paper_id"] for a in anchors],
                S_hint=S_hint,
                offsets=getattr(PipelineConfig, "DENSIFY_OFFSETS", [-0.5, 0.5, -0.25, 0.25]),
                max_total=getattr(PipelineConfig, "ANCHOR_MAX_TOTAL", 9)
            )
            if additional:
                anchors = anchors + additional
                anchors_rounds.append(additional)
                round2 = self._anchored_reviews(story, anchors, pattern_id)
            else:
                round2 = round1
        else:
            round2 = round1

        # Build final outputs
        reviews = round2['reviews']
        scores = round2['scores']
        avg_score = _safe_mean(scores)
        role_scores = {r['role']: r['score'] for r in reviews}
        passed, pass_audit = self._compute_pass_decision(avg_score, role_scores, pattern_id)
        main_issue, suggestions = self._diagnose_issue(reviews, scores)

        print("\n" + "-" * 80)
        print(f"ğŸ“Š è¯„å®¡ç»“æœ: å¹³å‡åˆ† {avg_score:.2f}/10 - {'âœ… PASS' if passed else 'âŒ FAIL'}")
        if not passed:
            print(f"ğŸ”§ ä¸»è¦é—®é¢˜: {main_issue}")
            print(f"ğŸ’¡ å»ºè®®: {', '.join(suggestions)}")
        print("=" * 80)

        audit = {
            'pattern_id': pattern_id,
            'anchors': anchors,
            'role_details': round2['role_details'],
            'anchors_rounds': anchors_rounds,
            'pass': pass_audit
        }

        self._log_event("pass_threshold_computed", {
            "pattern_id": pattern_id,
            "used_distribution": pass_audit.get("used_distribution"),
            "q50": pass_audit.get("q50"),
            "q75": pass_audit.get("q75"),
            "count_roles_ge_q75": pass_audit.get("count_roles_ge_q75"),
            "avg_score": avg_score,
            "passed": passed
        })

        return {
            'pass': passed,
            'avg_score': avg_score,
            'reviews': reviews,
            'main_issue': main_issue,
            'suggestions': suggestions,
            'audit': audit
        }

    def _single_review(self, story: Dict, reviewer: Dict) -> Dict:
        """å•ä¸ªè¯„å®¡å‘˜è¯„å®¡"""

        # é’ˆå¯¹ Novelty è§’è‰²çš„ç‰¹æ®ŠæŒ‡ä»¤
        special_instructions = ""
        if reviewer['role'] == 'Novelty':
            special_instructions = """
ã€ç‰¹åˆ«æ³¨æ„ã€‘
ä½œä¸º Novelty è¯„å®¡ï¼Œä½ éœ€è¦æ¯”è¾ƒä¸¥æ ¼ï¼Œä¸è¦è¢«è¡¨é¢çš„â€œæ–°é¢–â€è¯æ±‡è¿·æƒ‘ã€‚
1. **æ‰¹åˆ¤æ€§è¯„ä¼°ç»„åˆ**ï¼šä»”ç»†æ€è€ƒä½œè€…æå‡ºçš„æŠ€æœ¯æ˜¯å¦åœ¨è¿‘ä¸¤å¹´çš„ NLP/CV é¡¶ä¼šä¸­å·²ç»æ³›æ»¥ã€‚å¦‚æœæ˜¯å¸¸è§çš„â€œA+Bâ€å †ç Œä¸”ç¼ºä¹æ·±å±‚ç†è®ºåˆ›æ–°ï¼Œè¯·ç»™å‡ºä½åˆ†ï¼ˆ4-5åˆ†ï¼‰ã€‚
2. **æ‹’ç»å¹³åº¸**ï¼šå¦‚æœ Story åªæ˜¯å°†ç°æœ‰æŠ€æœ¯åº”ç”¨åˆ°æ–°é¢†åŸŸï¼ˆå¦‚â€œç”¨ BERT åš X ä»»åŠ¡â€ï¼‰ï¼Œè€Œæ²¡æœ‰é’ˆå¯¹è¯¥é¢†åŸŸçš„ç‹¬ç‰¹é€‚é…æˆ–ç†è®ºè´¡çŒ®ï¼Œè¿™ä¸å«åˆ›æ–°ã€‚
3. **ç›´è¨€ä¸è®³**ï¼šå¦‚æœå‘ç°æ˜¯å¸¸è§å¥—è·¯ï¼Œè¯·åœ¨åé¦ˆä¸­æ˜ç¡®æŒ‡å‡ºâ€œè¿™ç§ç»„åˆå·²ç»å¾ˆå¸¸è§â€æˆ–â€œç¼ºä¹å®è´¨æ€§åˆ›æ–°â€ã€‚
4. **é«˜åˆ†é—¨æ§›**ï¼šåªæœ‰çœŸæ­£çš„èŒƒå¼åˆ›æ–°ã€æå…·å¯å‘æ€§çš„åç›´è§‰å‘ç°ï¼Œæˆ–å¯¹ç°æœ‰æ–¹æ³•çš„æ ¹æœ¬æ€§æ”¹è¿›ï¼Œæ‰èƒ½å¾—åˆ° 8 åˆ†ä»¥ä¸Šã€‚
"""

        # NOTE: _single_review is kept for backward compatibility only.
        # For anchored review, use _anchored_review().
        problem_text = story.get('problem_framing') or story.get('problem_definition', '')
        method_text = story.get('method_skeleton', '')
        if isinstance(method_text, dict):
            method_text = ' '.join(str(v) for v in method_text.values() if v)

        # æ„å»º Prompt
        prompt = f"""
ä½ æ˜¯é¡¶çº§ NLP ä¼šè®®ï¼ˆå¦‚ ACL/ICLRï¼‰çš„**ä¸¥å‰è¯„å®¡ä¸“å®¶** {reviewer['name']}ï¼Œä¸“æ³¨äºè¯„ä¼°{reviewer['focus']}ã€‚
ä½ çš„æ‰“åˆ†æ ‡å‡†éå¸¸ä¸¥æ ¼ï¼Œæ»¡åˆ† 10 åˆ†ã€‚6 åˆ†ä»¥ä¸‹ä¸ºä¸åŠæ ¼ï¼ˆRejectï¼‰ï¼Œ8 åˆ†ä»¥ä¸Šä¸ºä¼˜ç§€ï¼ˆAcceptï¼‰ã€‚
{special_instructions}
è¯·è¯„å®¡ä»¥ä¸‹è®ºæ–‡ Storyï¼š

ã€æ ‡é¢˜ã€‘{story.get('title', '')}

ã€æ‘˜è¦ã€‘{story.get('abstract', '')}

ã€é—®é¢˜å®šä¹‰ã€‘{problem_text}

ã€æ–¹æ³•æ¦‚è¿°ã€‘{method_text}

ã€è´¡çŒ®ç‚¹ã€‘
{chr(10).join([f"  - {claim}" for claim in story.get('innovation_claims', [])])}

ã€å®éªŒè®¡åˆ’ã€‘{story.get('experiments_plan', '')}

è¯·ä»{reviewer['focus']}çš„è§’åº¦è¿›è¡Œè¯„å®¡ã€‚

ã€è¯„å®¡è¦æ±‚ã€‘
1. è¯·åˆ—å‡º 3 ä¸ªå…·ä½“çš„è¯„ä¼°ç»´åº¦ã€‚
2. **å¯¹æ¯ä¸ªç»´åº¦è¿›è¡Œæ‰“åˆ†ï¼ˆ1-10åˆ†ï¼‰**ï¼Œå¹¶ç»™å‡ºç†ç”±ã€‚
3. **æœ€ç»ˆæ€»åˆ†ï¼ˆscoreï¼‰å¿…é¡»æ˜¯å„ç»´åº¦åˆ†æ•°çš„ç»¼åˆè¯„ä¼°ï¼Œä¸¥ç¦å‡ºç°ç»†é¡¹åˆ†ä½ä½†æ€»åˆ†é«˜çš„æƒ…å†µã€‚**
4. å¦‚æœå‘ç°æ˜æ˜¾ç¼ºé™·ï¼ˆå¦‚åˆ›æ–°æ€§ä¸è¶³ã€æ–¹æ³•ä¸åˆç†ï¼‰ï¼Œè¯·ç»™å‡ºä½åˆ†ï¼ˆ<6åˆ†ï¼‰ã€‚

è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰ï¼š
{{
  "score": 6.5,
  "feedback": "1. ç»´åº¦A (6.0åˆ†): ç†ç”±...\\n2. ç»´åº¦B (7.0åˆ†): ç†ç”±...\\n\\næ€»ç»“: ..."
}}
"""

        # ä½¿ç”¨æ›´é•¿çš„è¶…æ—¶æ—¶é—´ï¼ˆ180 ç§’ï¼‰ä»¥åº”å¯¹ç½‘ç»œå»¶è¿Ÿ
        response = call_llm(prompt, temperature=0.3, max_tokens=800, timeout=180)

        # 1. å°è¯•æ ‡å‡† JSON è§£æ
        result = parse_json_from_llm(response)
        if result:
            return {
                'reviewer': reviewer['name'],
                'role': reviewer['role'],
                'score': float(result.get('score', 5.0)),
                'feedback': result.get('feedback', '')
            }

        print(f"   âš ï¸  JSON è§£æå¤±è´¥ï¼Œå°è¯• Fallback è§£æ")

        # 2. Fallback: æ­£åˆ™æå–åˆ†æ•°å’Œåé¦ˆ
        score = 5.0
        feedback = "è¯„å®¡æ„è§è§£æå¤±è´¥ï¼Œè¯·æŸ¥çœ‹åŸå§‹è¾“å‡º"

        # å°è¯•åŒ¹é…åˆ†æ•° "score": 7.5 æˆ– score: 7.5
        score_match = re.search(r'(?:\"|\')?score(?:\"|\')?\s*:\s*([\d\.]+)', response)
        if score_match:
            try:
                score = float(score_match.group(1))
                print(f"      ğŸ“Š ä»å“åº”ä¸­æå–åˆ†æ•°: {score}")
            except:
                pass

        # å°è¯•æå– feedback å­—æ®µï¼ˆæ›´åŠ å¥å£®ï¼‰
        # æ–¹æ³•1: åŒ¹é… "feedback": "..."
        feedback_match = re.search(
            r'(?:\"|\')?feedback(?:\"|\')?\s*:\s*"((?:[^"\\]|\\["\\/bfnrt]|\\u[0-9a-fA-F]{4})*)"',
            response,
            re.DOTALL
        )
        if feedback_match:
            feedback = feedback_match.group(1)
            feedback = feedback.replace('\\"', '"')
            feedback = feedback.replace('\\n', '\n')
            print(f"      ğŸ’¬ ä»å“åº”ä¸­æå– feedbackï¼ˆæ¨¡å¼1ï¼‰")
        else:
            # æ–¹æ³•2: æ›´å®½æ¾çš„åŒ¹é…
            feedback_match = re.search(
                r'(?:\"|\')?feedback(?:\"|\')?\s*:\s*"([^"]*(?:\\.[^"]*)*)"',
                response,
                re.DOTALL
            )
            if feedback_match:
                feedback = feedback_match.group(1)
                feedback = feedback.replace('\\"', '"')
                feedback = feedback.replace('\\n', '\n')
                print(f"      ğŸ’¬ ä»å“åº”ä¸­æå– feedbackï¼ˆæ¨¡å¼2ï¼‰")
            else:
                # æ–¹æ³•3: å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œå°è¯•æ‰¾åˆ°æ‰€æœ‰å†’å·åçš„å†…å®¹ï¼Œå–æœ€é•¿çš„
                content_matches = list(re.finditer(r':\s*"([^"]*(?:\\.[^"]*)*)"', response))
                if len(content_matches) >= 2:
                    # å‡è®¾ score æ˜¯ç¬¬ä¸€ä¸ªï¼Œfeedback æ˜¯ç¬¬äºŒä¸ª
                    feedback = content_matches[1].group(1)
                    feedback = feedback.replace('\\"', '"')
                    feedback = feedback.replace('\\n', '\n')
                    print(f"      ğŸ’¬ ä»å“åº”ä¸­æå– feedbackï¼ˆæ¨¡å¼3-å¯å‘å¼ï¼‰")
                else:
                    # æœ€åçš„å°è¯•ï¼šä½¿ç”¨åŸå§‹å“åº”çš„éƒ¨åˆ†å†…å®¹
                    print(f"      âš ï¸  æ— æ³•ç²¾ç¡®æå– feedbackï¼Œä½¿ç”¨åŸå§‹å“åº”æ‘˜å½•")

        return {
            'reviewer': reviewer['name'],
            'role': reviewer['role'],
            'score': score,
            'feedback': feedback
        }

    def _anchored_reviews(self, story: Dict, anchors: List[Dict], pattern_id: str) -> Dict:
        reviews = []
        scores = []
        role_details = {}
        for reviewer in self.reviewers:
            print(f"\nğŸ“ {reviewer['name']} ({reviewer['role']}) è¯„å®¡ä¸­...")
            anchored = self._anchored_review(story, reviewer, anchors, pattern_id)
            score = anchored['score']
            reviews.append({
                'reviewer': reviewer['name'],
                'role': reviewer['role'],
                'score': score,
                'feedback': anchored['feedback']
            })
            scores.append(score)
            role_details[reviewer['role']] = anchored['detail']

            print(f"   è¯„åˆ†: {score:.1f}/10")
            print(f"   åé¦ˆ: {anchored['feedback']}")

        return {
            'reviews': reviews,
            'scores': scores,
            'role_details': role_details
        }

    def _anchored_review(self, story: Dict, reviewer: Dict, anchors: List[Dict], pattern_id: str) -> Dict:
        """Anchored review: compare against real papers, then deterministically fit score."""
        comparisons, main_gaps = self._get_comparisons_with_retries(
            story=story,
            reviewer=reviewer,
            anchors=anchors,
            pattern_id=pattern_id
        )

        score, detail = self._compute_score_from_comparisons(anchors, comparisons)
        feedback = f"Main gaps: {', '.join(main_gaps[:3])}. Anchored against {len(anchors)} papers."

        return {
            'score': score,
            'feedback': feedback,
            'detail': {
                'comparisons': comparisons,
                'main_gaps': main_gaps,
                'score': score,
                **detail
            }
        }

    def _compute_score_from_comparisons(self, anchors: List[Dict], comparisons: List[Dict]) -> Tuple[float, Dict]:
        # Map paper_id -> comparison
        comp_map = {c.get('paper_id'): c for c in comparisons if c.get('paper_id')}
        probs = []
        weights = []
        scores = []
        confs = []

        for a in anchors:
            pid = a["paper_id"]
            comp = comp_map.get(pid, {"judgement": "tie", "confidence": 0.0})
            judgement = comp.get("judgement", "tie")
            confidence = float(comp.get("confidence", 0.0) or 0.0)
            confidence = max(0.0, min(1.0, confidence))
            if judgement == "better":
                p = 0.5 + 0.45 * confidence
            elif judgement == "worse":
                p = 0.5 - 0.45 * confidence
            else:
                p = 0.5
            probs.append(p)
            weights.append(float(a.get("weight", 1.0)))
            scores.append(float(a.get("score10", 5.0)))
            confs.append(confidence)

        # compute monotonic violations
        sorted_pairs = sorted(zip(scores, probs), key=lambda x: x[0])
        monotonic_violations = 0
        prev_p = None
        for s, p in sorted_pairs:
            if prev_p is not None and p > prev_p + 0.05:
                monotonic_violations += 1
            prev_p = p

        k = getattr(PipelineConfig, "SIGMOID_K", 1.2)
        step = getattr(PipelineConfig, "GRID_STEP", 0.01)
        best_s = 5.0
        best_loss = None
        S = 1.0
        while S <= 10.0 + 1e-9:
            loss = 0.0
            for p, w, s in zip(probs, weights, scores):
                pred = _sigmoid(k * (S - s))
                loss += w * (pred - p) ** 2
            if best_loss is None or loss < best_loss:
                best_loss = loss
                best_s = S
            S += step

        detail = {
            "loss": best_loss if best_loss is not None else 0.0,
            "avg_confidence": _safe_mean(confs),
            "monotonic_violations": monotonic_violations
        }
        return best_s, detail

    def _diagnose_issue(self, reviews: List[Dict], scores: List[float]) -> Tuple[str, List[str]]:
        """è¯Šæ–­ä¸»è¦é—®é¢˜

        Returns:
            (main_issue, suggestions)
        """
        # æ‰¾å‡ºåˆ†æ•°æœ€ä½çš„è¯„å®¡å‘˜
        min_idx = scores.index(min(scores))
        worst_review = reviews[min_idx]

        role = worst_review['role']

        # æ‰“å°è¯Šæ–­ä¿¡æ¯
        print(f"\n   ğŸ“Š è¯Šæ–­ä¿¡æ¯:")
        print(f"      åˆ†æ•°åˆ†å¸ƒ: {scores}")
        print(f"      æœ€ä½åˆ†è¯„å®¡å‘˜: {worst_review['reviewer']} ({role}), åˆ†æ•°: {scores[min_idx]}")

        # æ ¹æ®è§’è‰²è¯Šæ–­é—®é¢˜,æ˜ å°„åˆ°Patternåˆ†ç±»ç»´åº¦
        if role == 'Novelty':
            return 'novelty', ['ä»noveltyç»´åº¦é€‰æ‹©åˆ›æ–°Pattern', 'æ³¨å…¥é•¿å°¾Patternæå‡æ–°é¢–æ€§']
        elif role == 'Methodology':
            return 'stability', ['ä»stabilityç»´åº¦é€‰æ‹©ç¨³å¥Pattern', 'æ³¨å…¥æˆç†Ÿæ–¹æ³•å¢å¼ºé²æ£’æ€§']
        elif role == 'Storyteller':
            return 'domain_distance', ['ä»domain_distanceç»´åº¦é€‰æ‹©è·¨åŸŸPattern', 'å¼•å…¥ä¸åŒè§†è§’ä¼˜åŒ–å™äº‹']
        else:
            # Fallback
            return 'novelty', ['ä»noveltyç»´åº¦é€‰æ‹©åˆ›æ–°Pattern']
