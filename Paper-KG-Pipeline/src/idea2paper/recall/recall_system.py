"""
ä¸‰è·¯å¬å›ç³»ç»Ÿ Demo - Idea2Pattern (V3ç‰ˆæœ¬)

åŸºäºçŸ¥è¯†å›¾è°±çš„ä¸‰è·¯å¬å›ç­–ç•¥ï¼š
  è·¯å¾„1: Idea â†’ Idea â†’ Pattern (ç›¸ä¼¼Ideaå¬å›)
  è·¯å¾„2: Idea â†’ Domain â†’ Pattern (é¢†åŸŸç›¸å…³æ€§å¬å›)
  è·¯å¾„3: Idea â†’ Paper â†’ Pattern (ç›¸ä¼¼Paperå¬å›)

V3ç‰ˆæœ¬æ›´æ–°:
  - é€‚é…V3èŠ‚ç‚¹ç»“æ„ (Paper.ideaä¸ºå­—ç¬¦ä¸²ï¼ŒéåµŒå¥—å­—å…¸)
  - è·¯å¾„1ç›´æ¥ä½¿ç”¨Idea.pattern_idsï¼Œæ— éœ€é€šè¿‡Paperä¸­è½¬
  - Paperé€šè¿‡review_statsè·å–è´¨é‡åˆ†æ•°ï¼Œæ”¯æŒå…¼å®¹æ—§ç»“æ„
"""

import json
import os
import pickle
import time
import hashlib
from collections import defaultdict
from pathlib import Path
from typing import Dict, List, Tuple

import numpy as np
import requests

from pipeline.run_context import get_logger
from idea2paper.config import OUTPUT_DIR, PipelineConfig
from idea2paper.infra.embeddings import get_embeddings_batch, EMBEDDING_MODEL
from idea2paper.recall.recall_text import build_recall_idea_text, build_recall_paper_text, truncate_for_embedding
from idea2paper.recall.tokenize import to_token_set, jaccard_from_sets

# è¾“å…¥æ–‡ä»¶
NODES_IDEA = OUTPUT_DIR / "nodes_idea.json"
NODES_PATTERN = OUTPUT_DIR / "nodes_pattern.json"
NODES_DOMAIN = OUTPUT_DIR / "nodes_domain.json"
NODES_PAPER = OUTPUT_DIR / "nodes_paper.json"
EDGES_FILE = OUTPUT_DIR / "edges.json"
GRAPH_FILE = OUTPUT_DIR / "knowledge_graph_v2.gpickle"


# ===================== å¬å›å‚æ•°é…ç½® =====================
class RecallConfig:
    """å¬å›ç³»ç»Ÿé…ç½®"""
    # æ¯è·¯å¬å›çš„Top-K
    PATH1_TOP_K_IDEAS = 20       # è·¯å¾„1: å¬å›å‰Kä¸ªæœ€ç›¸ä¼¼çš„Idea
    PATH1_FINAL_TOP_K = 10       # è·¯å¾„1: æœ€ç»ˆåªä¿ç•™Top-Kä¸ªPatternï¼ˆé‡è¦é€šé“ï¼‰

    PATH2_TOP_K_DOMAINS = 5      # è·¯å¾„2: å¬å›å‰Kä¸ªæœ€ç›¸å…³çš„Domain
    PATH2_FINAL_TOP_K = 5        # è·¯å¾„2: æœ€ç»ˆåªä¿ç•™Top-Kä¸ªPatternï¼ˆè¾…åŠ©é€šé“ï¼‰

    PATH3_TOP_K_PAPERS = 20      # è·¯å¾„3: å¬å›å‰Kä¸ªæœ€ç›¸ä¼¼çš„Paper
    PATH3_FINAL_TOP_K = 10       # è·¯å¾„3: æœ€ç»ˆåªä¿ç•™Top-Kä¸ªPatternï¼ˆé‡è¦é€šé“ï¼‰

    # å„è·¯å¬å›çš„æƒé‡
    PATH1_WEIGHT = 0.4  # è·¯å¾„1æƒé‡ï¼ˆç›¸ä¼¼Idea - é‡è¦ï¼‰
    PATH2_WEIGHT = 0.2  # è·¯å¾„2æƒé‡ï¼ˆé¢†åŸŸç›¸å…³ - è¾…åŠ©ï¼‰
    PATH3_WEIGHT = 0.4  # è·¯å¾„3æƒé‡ï¼ˆç›¸ä¼¼Paper - é‡è¦ï¼‰

    # æœ€ç»ˆå¬å›çš„Top-K
    FINAL_TOP_K = 10

    # ç›¸ä¼¼åº¦è®¡ç®—æ–¹å¼
    USE_EMBEDDING = True  # ä½¿ç”¨embeddingè®¡ç®—ç›¸ä¼¼åº¦ï¼ˆæ¨èï¼‰ï¼ŒFalseåˆ™ä½¿ç”¨Jaccard

    # ä¸¤é˜¶æ®µå¬å›ä¼˜åŒ–ï¼ˆç²—æ’+ç²¾æ’ï¼‰
    TWO_STAGE_RECALL = True      # å¯ç”¨ä¸¤é˜¶æ®µå¬å›ï¼ˆå¤§å¹…æé€Ÿï¼‰
    COARSE_RECALL_SIZE = 100     # ç²—å¬å›æ•°é‡ï¼ˆJaccardå¿«é€Ÿç­›é€‰ï¼‰
    FINE_RECALL_SIZE = 20        # ç²¾æ’æ•°é‡ï¼ˆEmbeddingç²¾ç¡®æ’åºï¼‰


# ===================== å¬å›ç³»ç»Ÿ =====================
class RecallSystem:
    """ä¸‰è·¯å¬å›ç³»ç»Ÿ"""

    def __init__(self, logger=None):
        print("ğŸš€ åˆå§‹åŒ–å¬å›ç³»ç»Ÿ...")
        self.logger = logger or get_logger()

        # åŠ è½½æ•°æ®
        self.ideas = self._load_json(NODES_IDEA)
        self.patterns = self._load_json(NODES_PATTERN)
        self.domains = self._load_json(NODES_DOMAIN)
        self.papers = self._load_json(NODES_PAPER)

        # åŠ è½½å›¾è°±
        with open(GRAPH_FILE, 'rb') as f:
            self.G = pickle.load(f)

        # æ„å»ºç´¢å¼•
        self.idea_id_to_idea = {i['idea_id']: i for i in self.ideas}
        self.pattern_id_to_pattern = {p['pattern_id']: p for p in self.patterns}
        self.domain_id_to_domain = {d['domain_id']: d for d in self.domains}
        self.paper_id_to_paper = {p['paper_id']: p for p in self.papers}

        self._use_embed_batch = True
        self._use_token_cache = True
        self._use_offline_index = bool(PipelineConfig.RECALL_USE_OFFLINE_INDEX)
        self._embed_batch_size = int(PipelineConfig.RECALL_EMBED_BATCH_SIZE)
        self._embed_max_retries = int(PipelineConfig.RECALL_EMBED_MAX_RETRIES)
        self._embed_sleep_sec = float(PipelineConfig.RECALL_EMBED_SLEEP_SEC)
        self._recall_index_dir = Path(PipelineConfig.RECALL_INDEX_DIR)

        self._offline_index_loaded = False
        self._offline_index_ok = False
        self._offline_index_reason = None
        self._idea_emb = None
        self._idea_meta = None
        self._idea_id_to_idx = {}
        self._paper_emb = None
        self._paper_meta = None
        self._paper_id_to_idx = {}

        self._idea_token_sets = {}
        self._paper_token_sets = {}
        if self._use_token_cache:
            for idea in self.ideas:
                idea_id = idea.get("idea_id")
                if idea_id:
                    self._idea_token_sets[idea_id] = to_token_set(build_recall_idea_text(idea))
            for paper in self.papers:
                paper_id = paper.get("paper_id")
                if paper_id:
                    self._paper_token_sets[paper_id] = to_token_set(build_recall_paper_text(paper))

        print(f"  âœ“ åŠ è½½ {len(self.ideas)} ä¸ªIdea")
        print(f"  âœ“ åŠ è½½ {len(self.patterns)} ä¸ªPattern")
        print(f"  âœ“ åŠ è½½ {len(self.domains)} ä¸ªDomain")
        print(f"  âœ“ åŠ è½½ {len(self.papers)} ä¸ªPaper")
        print(f"  âœ“ å›¾è°±èŠ‚ç‚¹: {self.G.number_of_nodes()}, è¾¹: {self.G.number_of_edges()}")
        print()

    def _load_json(self, filepath: Path) -> List[Dict]:
        """åŠ è½½JSONæ–‡ä»¶"""
        with open(filepath, 'r', encoding='utf-8') as f:
            return json.load(f)

    def _file_hash(self, path: Path) -> str:
        h = hashlib.sha256()
        with path.open("rb") as f:
            for chunk in iter(lambda: f.read(1024 * 1024), b""):
                h.update(chunk)
        return h.hexdigest()

    def _load_index_kind(self, kind: str, emb_path: Path, meta_path: Path, manifest_path: Path, expected_hash: str):
        if not emb_path.exists() or not meta_path.exists() or not manifest_path.exists():
            return None
        try:
            manifest = json.loads(manifest_path.read_text(encoding="utf-8"))
            if manifest.get("embedding_model") != EMBEDDING_MODEL:
                return None
            if manifest.get(f"nodes_{kind}_hash") != expected_hash:
                return None
            emb = np.load(emb_path)
            meta = [json.loads(l) for l in meta_path.read_text(encoding="utf-8").splitlines() if l.strip()]
            id_key = f"{kind}_id"
            id_to_idx = {m.get(id_key): i for i, m in enumerate(meta) if m.get(id_key)}
            return {"emb": emb, "meta": meta, "id_to_idx": id_to_idx, "manifest": manifest}
        except Exception:
            return None

    def _load_offline_index(self) -> bool:
        if not self._use_offline_index:
            return False
        if self._offline_index_loaded:
            return self._offline_index_ok

        self._offline_index_loaded = True
        self._offline_index_ok = False

        idea_manifest = self._recall_index_dir / "idea_manifest.json"
        idea_emb = self._recall_index_dir / "idea_emb.npy"
        idea_meta = self._recall_index_dir / "idea_meta.jsonl"

        paper_manifest = self._recall_index_dir / "paper_manifest.json"
        paper_emb = self._recall_index_dir / "paper_emb.npy"
        paper_meta = self._recall_index_dir / "paper_meta.jsonl"

        idea_hash = self._file_hash(NODES_IDEA) if NODES_IDEA.exists() else None
        paper_hash = self._file_hash(NODES_PAPER) if NODES_PAPER.exists() else None

        idea_idx = self._load_index_kind("idea", idea_emb, idea_meta, idea_manifest, idea_hash)
        paper_idx = self._load_index_kind("paper", paper_emb, paper_meta, paper_manifest, paper_hash)

        if not idea_idx or not paper_idx:
            self._offline_index_reason = "missing_or_mismatch"
            if self.logger:
                self.logger.log_event("recall_offline_index_fallback", {
                    "reason": self._offline_index_reason,
                    "index_dir": str(self._recall_index_dir),
                })
            return False

        self._idea_emb = idea_idx["emb"]
        self._idea_meta = idea_idx["meta"]
        self._idea_id_to_idx = idea_idx["id_to_idx"]
        self._paper_emb = paper_idx["emb"]
        self._paper_meta = paper_idx["meta"]
        self._paper_id_to_idx = paper_idx["id_to_idx"]
        self._offline_index_ok = True
        if self.logger:
            self.logger.log_event("recall_offline_index_used", {
                "index_dir": str(self._recall_index_dir),
                "idea_manifest": idea_idx["manifest"],
                "paper_manifest": paper_idx["manifest"],
            })
        return True

    def _get_offline_embeddings(self, kind: str, ids: List[str]):
        if not self._load_offline_index():
            return None
        if kind == "idea":
            id_to_idx = self._idea_id_to_idx
            emb = self._idea_emb
        else:
            id_to_idx = self._paper_id_to_idx
            emb = self._paper_emb
        idxs = []
        for _id in ids:
            idx = id_to_idx.get(_id)
            if idx is None:
                return None
            idxs.append(idx)
        return emb[np.array(idxs, dtype=int)]

    def _cosine_scores(self, query_emb: np.ndarray, cand_embs: np.ndarray) -> List[float]:
        # Use float64 to minimize numeric drift vs. per-item cosine computation.
        q = np.array(query_emb, dtype=float)
        c = np.array(cand_embs, dtype=float)
        q_norm = np.linalg.norm(q)
        c_norms = np.linalg.norm(c, axis=1)
        c_norms[c_norms == 0] = 1.0
        if q_norm == 0:
            return [0.0 for _ in range(c.shape[0])]
        scores = (c @ q) / (c_norms * q_norm)
        return [float(s) for s in scores]

    def _batch_embeddings(self, texts: List[str]):
        if not texts:
            return []
        payload = [truncate_for_embedding(t) for t in texts]
        for attempt in range(self._embed_max_retries + 1):
            embs = get_embeddings_batch(payload, logger=self.logger, timeout=10)
            if embs is not None:
                return embs
            time.sleep(self._embed_sleep_sec * (attempt + 1))
        return None

    def _compute_embedding_similarities(self, user_idea: str, candidate_ids: List[str], kind: str) -> List[Tuple[str, float]]:
        if kind == "idea":
            texts = [build_recall_idea_text(self.idea_id_to_idea[i]) for i in candidate_ids]
        else:
            texts = [build_recall_paper_text(self.paper_id_to_paper[i]) for i in candidate_ids]

        if not self._use_embed_batch:
            return [(cid, self._compute_embedding_similarity(user_idea, text)) for cid, text in zip(candidate_ids, texts)]

        query_emb = self._get_embedding(truncate_for_embedding(user_idea))
        if query_emb is None:
            return [(cid, self._compute_jaccard_similarity(user_idea, text)) for cid, text in zip(candidate_ids, texts)]

        cand_embs = None
        if self._use_offline_index:
            cand_embs = self._get_offline_embeddings(kind, candidate_ids)
            if cand_embs is None and self.logger:
                self.logger.log_event("recall_offline_index_fallback", {
                    "reason": self._offline_index_reason or "missing_candidate",
                    "index_dir": str(self._recall_index_dir),
                })

        if cand_embs is None:
            cand_embs = self._batch_embeddings(texts)
            if cand_embs is None:
                return [(cid, self._compute_embedding_similarity(user_idea, text)) for cid, text in zip(candidate_ids, texts)]

        scores = self._cosine_scores(query_emb, cand_embs)
        return [(cid, sim) for cid, sim in zip(candidate_ids, scores)]

    def _compute_text_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦

        æ”¯æŒä¸¤ç§æ¨¡å¼:
        1. USE_EMBEDDING=True: ä½¿ç”¨Qwen3-Embedding-4Bè®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆæ¨èï¼‰
        2. USE_EMBEDDING=False: ä½¿ç”¨è¯è¢‹Jaccardç›¸ä¼¼åº¦ï¼ˆå¿«é€Ÿä½†ä¸å‡†ç¡®ï¼‰
        """
        if not text1 or not text2:
            return 0.0

        if RecallConfig.USE_EMBEDDING:
            return self._compute_embedding_similarity(text1, text2)
        else:
            return self._compute_jaccard_similarity(text1, text2)

    def _compute_jaccard_similarity(self, text1: str, text2: str) -> float:
        """è¯è¢‹Jaccardç›¸ä¼¼åº¦ï¼ˆå¿«é€Ÿä½†ä¸å‡†ç¡®ï¼‰"""
        tokens1 = to_token_set(text1)
        tokens2 = to_token_set(text2)
        return jaccard_from_sets(tokens1, tokens2)

    def _compute_embedding_similarity(self, text1: str, text2: str) -> float:
        """åŸºäºembeddingçš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆæ›´å‡†ç¡®ï¼‰"""
        # è·å–ä¸¤ä¸ªæ–‡æœ¬çš„embedding
        emb1 = self._get_embedding(text1)
        emb2 = self._get_embedding(text2)

        if emb1 is None or emb2 is None:
            # é™çº§åˆ°Jaccardç›¸ä¼¼åº¦
            return self._compute_jaccard_similarity(text1, text2)

        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        emb1 = np.array(emb1)
        emb2 = np.array(emb2)

        cosine_sim = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))
        return float(cosine_sim)

    def _get_embedding(self, text: str, max_retries: int = 3) -> List[float]:
        """è°ƒç”¨SiliconFlow APIè·å–æ–‡æœ¬embedding"""
        api_key = os.environ.get('SILICONFLOW_API_KEY', '')

        if not api_key:
            if not hasattr(self, '_embedding_warning_shown'):
                print("  âš ï¸  æœªè®¾ç½®SILICONFLOW_API_KEYï¼Œé™çº§åˆ°Jaccardç›¸ä¼¼åº¦")
                self._embedding_warning_shown = True
            return None

        url = "https://api.siliconflow.cn/v1/embeddings"
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }

        payload = {
            "model": EMBEDDING_MODEL,
            "input": truncate_for_embedding(text)
        }

        for attempt in range(max_retries):
            try:
                start_ts = time.time()
                response = requests.post(url, headers=headers, json=payload, timeout=10)
                response.raise_for_status()
                result = response.json()
                if self.logger:
                    self.logger.log_embedding_call(
                        request={
                            "provider": "siliconflow",
                            "url": url,
                            "model": payload["model"],
                            "input_preview": truncate_for_embedding(text),
                            "timeout": 10
                        },
                        response={
                            "ok": True,
                            "latency_ms": int((time.time() - start_ts) * 1000)
                        }
                    )
                return result['data'][0]['embedding']
            except Exception as e:
                if attempt < max_retries - 1:
                    time.sleep(0.5)
                else:
                    if not hasattr(self, '_embedding_error_shown'):
                        print(f"  âš ï¸  Embedding APIè°ƒç”¨å¤±è´¥: {e}ï¼Œé™çº§åˆ°Jaccardç›¸ä¼¼åº¦")
                        self._embedding_error_shown = True
                    if self.logger:
                        self.logger.log_embedding_call(
                            request={
                                "provider": "siliconflow",
                            "url": url,
                            "model": payload["model"],
                            "input_preview": truncate_for_embedding(text),
                            "timeout": 10
                        },
                            response={
                                "ok": False,
                                "latency_ms": 0,
                                "error": str(e)
                            }
                        )
                    return None

        return None

    def _get_paper_quality(self, paper: Dict) -> float:
        """è®¡ç®—Paperçš„ç»¼åˆè´¨é‡åˆ†æ•°

        åŸºäºreviewçš„è¯„åˆ†ï¼Œå½’ä¸€åŒ–åˆ°[0, 1]
        å¦‚æœæ²¡æœ‰reviewæ•°æ®ï¼Œè¿”å›é»˜è®¤å€¼0.5
        """
        # ä¼˜å…ˆä½¿ç”¨æ–°ç»“æ„ä¸­çš„ review_stats.avg_score
        review_stats = paper.get('review_stats', {})

        if review_stats and review_stats.get('avg_score'):
            # å·²ç»æ˜¯ 0-1 çš„åˆ†æ•°
            return float(review_stats['avg_score'])

        # å¤‡é€‰æ–¹æ¡ˆï¼šå…¼å®¹æ—§ç»“æ„ï¼ˆreview åˆ—è¡¨ï¼‰
        reviews = paper.get('reviews', [])

        if not reviews:
            return 0.5  # é»˜è®¤ä¸­ç­‰è´¨é‡

        # æå–æ‰€æœ‰è¯„åˆ†
        scores = []
        for review in reviews:
            score_str = review.get('overall_score', '')
            # å°è¯•è§£æè¯„åˆ†ï¼ˆå¯èƒ½æ˜¯ "7", "7/10", "7.0" ç­‰æ ¼å¼ï¼‰
            try:
                if '/' in score_str:
                    score_str = score_str.split('/')[0]
                score = float(score_str.strip())
                scores.append(score)
            except (ValueError, AttributeError):
                continue

        if not scores:
            return 0.5

        # è®¡ç®—å¹³å‡åˆ†å¹¶å½’ä¸€åŒ–
        import numpy as np
        avg_score = np.mean(scores)
        # å‡è®¾è¯„åˆ†èŒƒå›´æ˜¯ 1-10ï¼Œå½’ä¸€åŒ–åˆ° [0, 1]
        normalized_score = (avg_score - 1) / 9

        return min(max(normalized_score, 0.0), 1.0)

    # ===================== è·¯å¾„1: Idea â†’ Idea â†’ Pattern =====================

    def _recall_path1_similar_ideas(self, user_idea: str) -> Tuple[Dict[str, float], List[Tuple[str, float]]]:
        """è·¯å¾„1: é€šè¿‡ç›¸ä¼¼Ideaå¬å›Pattern (V3ç‰ˆæœ¬ + ä¸¤é˜¶æ®µä¼˜åŒ–)

        æµç¨‹:
          1. ã€ç²—æ’ã€‘ä½¿ç”¨Jaccardå¿«é€Ÿç­›é€‰Top-Nå€™é€‰ï¼ˆN=100ï¼‰
          2. ã€ç²¾æ’ã€‘å¯¹å€™é€‰ä½¿ç”¨Embeddingé‡æ–°æ’åºï¼Œé€‰æ‹©Top-Kï¼ˆK=10ï¼‰
          3. ç›´æ¥è·å–è¿™äº›Ideaçš„pattern_ids
          4. æŒ‰ç›¸ä¼¼åº¦åŠ æƒè®¡ç®—Patternå¾—åˆ†

        è¿”å›: (pattern_scores, top_ideas)
            - pattern_scores: {pattern_id: score}
            - top_ideas: [(idea_id, similarity), ...] ç”¨äºè·¯å¾„2çš„DomainæŸ¥æ‰¾
        """
        print("\nğŸ” [è·¯å¾„1] ç›¸ä¼¼Ideaå¬å›...")

        # Step 1: ç²—æ’ - ä½¿ç”¨Jaccardå¿«é€Ÿç­›é€‰
        if RecallConfig.TWO_STAGE_RECALL and RecallConfig.USE_EMBEDDING:
            print(f"  [ç²—æ’] ä½¿ç”¨Jaccardå¿«é€Ÿç­›é€‰Top-{RecallConfig.COARSE_RECALL_SIZE}...")
            coarse_similarities = []
            user_tokens = to_token_set(user_idea)
            for idea in self.ideas:
                idea_id = idea.get("idea_id")
                if self._use_token_cache and idea_id in self._idea_token_sets:
                    sim = jaccard_from_sets(user_tokens, self._idea_token_sets[idea_id])
                else:
                    sim = self._compute_jaccard_similarity(user_idea, idea.get('description', ''))
                if sim > 0:
                    coarse_similarities.append((idea['idea_id'], sim))

            coarse_similarities.sort(key=lambda x: x[1], reverse=True)
            candidates = coarse_similarities[:RecallConfig.COARSE_RECALL_SIZE]
            self._last_path3_candidates = candidates
            self._last_path1_candidates = candidates

            print(f"  [ç²¾æ’] ä½¿ç”¨Embeddingé‡æ’Top-{RecallConfig.FINE_RECALL_SIZE}...")
            # Step 2: ç²¾æ’ - å¯¹å€™é€‰ä½¿ç”¨Embeddingé‡æ–°è®¡ç®—
            fine_similarities = []
            candidate_ids = [idea_id for idea_id, _ in candidates]
            sims = self._compute_embedding_similarities(user_idea, candidate_ids, kind="idea")
            for idea_id, sim in sims:
                if sim > 0:
                    fine_similarities.append((idea_id, sim))

            fine_similarities.sort(key=lambda x: x[1], reverse=True)
            top_ideas = fine_similarities[:RecallConfig.PATH1_TOP_K_IDEAS]
            self._last_path1_top_ideas = top_ideas

            print(f"  âœ“ ç²—æ’{len(coarse_similarities)}ä¸ª â†’ ç²¾æ’{len(candidates)}ä¸ª â†’ æœ€ç»ˆ{len(top_ideas)}ä¸ª")
        else:
            # å•é˜¶æ®µå¬å›ï¼ˆåŸé€»è¾‘ï¼‰
            similarities = []
            for idea in self.ideas:
                sim = self._compute_text_similarity(user_idea, idea['description'])
                if sim > 0:
                    similarities.append((idea['idea_id'], sim))

            similarities.sort(key=lambda x: x[1], reverse=True)
            top_ideas = similarities[:RecallConfig.PATH1_TOP_K_IDEAS]
            self._last_path1_candidates = similarities[:RecallConfig.COARSE_RECALL_SIZE]
            self._last_path1_top_ideas = top_ideas
            print(f"  æ‰¾åˆ° {len(similarities)} ä¸ªç›¸ä¼¼Ideaï¼Œé€‰æ‹©Top-{RecallConfig.PATH1_TOP_K_IDEAS}")

        # Step 3: ç›´æ¥ä»IdeaèŠ‚ç‚¹è·å–pattern_idså¹¶è®¡ç®—å¾—åˆ†
        pattern_scores = defaultdict(float)

        for idea_id, similarity in top_ideas:
            idea = self.idea_id_to_idea[idea_id]
            pattern_ids = idea.get('pattern_ids', [])

            # æ‰“å°Ideaçš„å‰300ä¸ªå­—ç¬¦ç”¨äºè°ƒè¯•
            idea_desc = idea.get('description', '')[:300]
            print(f"  - [{idea_id}] {idea_desc}... (ç›¸ä¼¼åº¦={similarity:.3f}, {len(pattern_ids)}ä¸ªPattern)")

            # V3ç‰ˆæœ¬: ç›´æ¥ä½¿ç”¨IdeaèŠ‚ç‚¹ä¸­çš„pattern_ids
            for pattern_id in pattern_ids:
                # å¾—åˆ† = ç›¸ä¼¼åº¦ (Paperè´¨é‡æš‚æ—¶é»˜è®¤0.5ï¼Œå·²é›†æˆåœ¨ç›¸ä¼¼åº¦ä¸­)
                pattern_scores[pattern_id] += similarity

        # æ’åºå¹¶åªä¿ç•™Top-Kä¸ªPattern
        sorted_patterns = sorted(pattern_scores.items(), key=lambda x: x[1], reverse=True)
        top_patterns = dict(sorted_patterns[:RecallConfig.PATH1_FINAL_TOP_K])

        print(f"  âœ“ å¬å› {len(pattern_scores)} ä¸ªPatternï¼Œä¿ç•™Top-{RecallConfig.PATH1_FINAL_TOP_K}")
        return top_patterns, top_ideas

    # ===================== è·¯å¾„2: Idea â†’ Domain â†’ Pattern =====================

    def _recall_path2_domain_patterns(self, user_idea: str, top_ideas: List[Tuple[str, float]] = None) -> Dict[str, float]:
        """è·¯å¾„2: é€šè¿‡é¢†åŸŸç›¸å…³æ€§å¬å›Pattern

        æµç¨‹:
          1. ä½¿ç”¨è·¯å¾„1å¬å›çš„ Top-1 Idea çš„ Domain
          2. åœ¨è¿™äº›Domainä¸­æ‰¾åˆ°è¡¨ç°å¥½çš„Pattern
          3. æŒ‰Domainç›¸å…³æ€§å’ŒPatternæ•ˆæœåŠ æƒè®¡ç®—å¾—åˆ†

        Args:
            user_idea: ç”¨æˆ·è¾“å…¥çš„Ideaæè¿°
            top_ideas: è·¯å¾„1å¬å›çš„Top Ideas [(idea_id, similarity), ...]

        è¿”å›: {pattern_id: score}
        """
        print("\nğŸŒ [è·¯å¾„2] é¢†åŸŸç›¸å…³æ€§å¬å›...")

        # Step 1: é€šè¿‡æœ€ç›¸ä¼¼Ideaçš„Domainï¼ˆä¸ simple_recall_demo.py ä¸€è‡´ï¼‰
        domain_scores = []

        # å¦‚æœæä¾›äº†top_ideasï¼Œä½¿ç”¨Top-1 Ideaçš„Domain
        if top_ideas:
            top_idea_id = top_ideas[0][0]
            top_idea = self.idea_id_to_idea.get(top_idea_id)

            if top_idea and self.G.has_node(top_idea['idea_id']):
                for successor in self.G.successors(top_idea['idea_id']):
                    edge_data = self.G[top_idea['idea_id']][successor]
                    if edge_data.get('relation') == 'belongs_to':
                        domain_id = successor
                        weight = edge_data.get('weight', 0.5)
                        domain_scores.append((domain_id, weight))

        # Fallback: å¦‚æœæ²¡æœ‰æ‰¾åˆ°Domainï¼Œé‡æ–°è®¡ç®—æœ€ç›¸ä¼¼çš„Idea
        if not domain_scores:
            print("  æœªæ‰¾åˆ°ç›´æ¥å…³è”çš„Domainï¼Œé‡æ–°è®¡ç®—æœ€ç›¸ä¼¼Idea...")
            similarities = []
            for idea in self.ideas:
                sim = self._compute_text_similarity(user_idea, idea['description'])
                if sim > 0:
                    similarities.append((idea, sim))

            similarities.sort(key=lambda x: x[1], reverse=True)
            top_idea = similarities[0][0] if similarities else None

            if top_idea:
                # é€šè¿‡å›¾è°±æ‰¾åˆ°Ideaçš„Domain
                for successor in self.G.successors(top_idea['idea_id']):
                    edge_data = self.G[top_idea['idea_id']][successor]
                    if edge_data.get('relation') == 'belongs_to':
                        domain_id = successor
                        weight = edge_data.get('weight', 0.5)
                        domain_scores.append((domain_id, weight))

        # Step 2: æ’åºå¹¶é€‰æ‹©Top-K Domain
        domain_scores.sort(key=lambda x: x[1], reverse=True)
        top_domains = domain_scores[:RecallConfig.PATH2_TOP_K_DOMAINS]
        # ç¼“å­˜ç”¨äºå®¡è®¡
        self._last_path2_top_domains = top_domains

        print(f"  æ‰¾åˆ° {len(domain_scores)} ä¸ªç›¸å…³Domainï¼Œé€‰æ‹©Top-{RecallConfig.PATH2_TOP_K_DOMAINS}")

        # Step 3: ä»è¿™äº›Domainä¸­æ‰¾Pattern
        pattern_scores = defaultdict(float)

        for domain_id, domain_weight in top_domains:
            domain = self.domain_id_to_domain.get(domain_id)
            if not domain:
                continue

            # æ‰“å°Domainè¯¦ç»†ä¿¡æ¯
            domain_name = domain.get('name', 'N/A')
            paper_count = domain.get('paper_count', 0)
            sub_domains = domain.get('sub_domains', [])
            sub_domain_str = ', '.join(sub_domains[:5])  # åªæ˜¾ç¤ºå‰5ä¸ªsub_domain
            if len(sub_domains) > 5:
                sub_domain_str += f"... (å…±{len(sub_domains)}ä¸ª)"

            print(f"  - {domain_id} (åç§°={domain_name}, ç›¸å…³åº¦={domain_weight:.3f}, è®ºæ–‡æ•°={paper_count})")
            if sub_domain_str:
                print(f"    å­é¢†åŸŸ: {sub_domain_str}")

            # æ‰¾åˆ°åœ¨è¯¥Domainä¸­è¡¨ç°å¥½çš„Pattern
            for predecessor in self.G.predecessors(domain_id):
                edge_data = self.G[predecessor][domain_id]
                if edge_data.get('relation') == 'works_well_in':
                    pattern_id = predecessor
                    effectiveness = edge_data.get('effectiveness', 0.0)
                    confidence = edge_data.get('confidence', 0.0)

                    # å¾—åˆ† = Domainç›¸å…³åº¦ Ã— æ•ˆæœ Ã— ç½®ä¿¡åº¦
                    score = domain_weight * max(effectiveness, 0.1) * confidence
                    pattern_scores[pattern_id] += score

        # æ’åºå¹¶åªä¿ç•™Top-Kä¸ªPatternï¼ˆé¿å…å¬å›è¿‡å¤šï¼‰
        sorted_patterns = sorted(pattern_scores.items(), key=lambda x: x[1], reverse=True)
        top_patterns = dict(sorted_patterns[:RecallConfig.PATH2_FINAL_TOP_K])

        print(f"  âœ“ å¬å› {len(pattern_scores)} ä¸ªPatternï¼Œä¿ç•™Top-{RecallConfig.PATH2_FINAL_TOP_K}")
        return top_patterns

    # ===================== è·¯å¾„3: Idea â†’ Paper â†’ Pattern =====================

    def _recall_path3_similar_papers(self, user_idea: str) -> Dict[str, float]:
        """è·¯å¾„3: é€šè¿‡ç›¸ä¼¼Paperå¬å›Pattern (V3ç‰ˆæœ¬ + ä¸¤é˜¶æ®µä¼˜åŒ–)

        æµç¨‹:
          1. ã€ç²—æ’ã€‘ä½¿ç”¨Jaccardå¿«é€Ÿç­›é€‰Top-Nå€™é€‰ï¼ˆN=100ï¼‰
          2. ã€ç²¾æ’ã€‘å¯¹å€™é€‰ä½¿ç”¨Embeddingé‡æ–°æ’åºï¼Œé€‰æ‹©Top-Kï¼ˆK=20ï¼‰
          3. æ”¶é›†è¿™äº›Paperä½¿ç”¨çš„Pattern
          4. æŒ‰Paperç›¸ä¼¼åº¦å’Œè´¨é‡åŠ æƒè®¡ç®—å¾—åˆ†

        æ³¨æ„:
          - ä½¿ç”¨Paperçš„titleè¿›è¡Œç›¸ä¼¼åº¦è®¡ç®—(ä¸è·¯å¾„1çš„idea descriptionäº’è¡¥)
          - V3ç‰ˆæœ¬Paperæš‚æ— reviewæ•°æ®æ—¶ï¼Œè´¨é‡é»˜è®¤0.5

        è¿”å›: {pattern_id: score}
        """
        print("\nğŸ“„ [è·¯å¾„3] ç›¸ä¼¼Paperå¬å›...")

        # Step 1: ç²—æ’ - ä½¿ç”¨Jaccardå¿«é€Ÿç­›é€‰
        if RecallConfig.TWO_STAGE_RECALL and RecallConfig.USE_EMBEDDING:
            print(f"  [ç²—æ’] ä½¿ç”¨Jaccardå¿«é€Ÿç­›é€‰Top-{RecallConfig.COARSE_RECALL_SIZE}...")
            coarse_similarities = []
            user_tokens = to_token_set(user_idea)

            for paper in self.papers:
                paper_title = paper.get('title', '')
                if not paper_title:
                    continue

                paper_id = paper.get("paper_id")
                if self._use_token_cache and paper_id in self._paper_token_sets:
                    sim = jaccard_from_sets(user_tokens, self._paper_token_sets[paper_id])
                else:
                    sim = self._compute_jaccard_similarity(user_idea, paper_title)
                if sim > 0.05:  # é™ä½é˜ˆå€¼ä»¥ä¿ç•™æ›´å¤šå€™é€‰
                    coarse_similarities.append((paper['paper_id'], sim))

            coarse_similarities.sort(key=lambda x: x[1], reverse=True)
            candidates = coarse_similarities[:RecallConfig.COARSE_RECALL_SIZE]

            print(f"  [ç²¾æ’] ä½¿ç”¨Embeddingé‡æ’Top-{RecallConfig.PATH3_TOP_K_PAPERS}...")
            # Step 2: ç²¾æ’ - å¯¹å€™é€‰ä½¿ç”¨Embeddingé‡æ–°è®¡ç®—
            fine_similarities = []
            candidate_ids = [paper_id for paper_id, _ in candidates]
            sims = self._compute_embedding_similarities(user_idea, candidate_ids, kind="paper")
            for paper_id, sim in sims:
                if sim > 0.1:  # è¿‡æ»¤ä½ç›¸ä¼¼åº¦
                    paper = self.paper_id_to_paper[paper_id]
                    quality = self._get_paper_quality(paper)
                    combined_weight = sim * quality
                    fine_similarities.append((paper_id, sim, quality, combined_weight))

            fine_similarities.sort(key=lambda x: x[3], reverse=True)
            top_papers = fine_similarities[:RecallConfig.PATH3_TOP_K_PAPERS]

            print(f"  âœ“ ç²—æ’{len(coarse_similarities)}ä¸ª â†’ ç²¾æ’{len(candidates)}ä¸ª â†’ æœ€ç»ˆ{len(top_papers)}ä¸ª")
        else:
            # å•é˜¶æ®µå¬å›ï¼ˆåŸé€»è¾‘ï¼‰
            similarities = []

            for paper in self.papers:
                paper_title = paper.get('title', '')
                if not paper_title:
                    continue

                sim = self._compute_text_similarity(user_idea, paper_title)
                if sim > 0.1:  # è¿‡æ»¤ä½ç›¸ä¼¼åº¦
                    quality = self._get_paper_quality(paper)
                    combined_weight = sim * quality
                    similarities.append((paper['paper_id'], sim, quality, combined_weight))

            similarities.sort(key=lambda x: x[3], reverse=True)
            top_papers = similarities[:RecallConfig.PATH3_TOP_K_PAPERS]
            self._last_path3_candidates = similarities[:RecallConfig.COARSE_RECALL_SIZE]

            print(f"  æ‰¾åˆ° {len(similarities)} ä¸ªç›¸ä¼¼Paperï¼Œé€‰æ‹©Top-{RecallConfig.PATH3_TOP_K_PAPERS}")

        # ç¼“å­˜ç”¨äºå®¡è®¡
        self._last_path3_top_papers = top_papers

        # Step 3: æ”¶é›†Pattern
        pattern_scores = defaultdict(float)

        for paper_id, similarity, quality, combined_weight in top_papers:
            paper = self.paper_id_to_paper.get(paper_id, {})
            # åˆ¤æ–­è´¨é‡æ¥æºï¼šä¼˜å…ˆæ£€æŸ¥review_statsï¼Œç„¶åæ˜¯reviewsï¼Œå¦åˆ™æ˜¯é»˜è®¤å€¼
            if paper.get('review_stats'):
                quality_source = f"review({paper['review_stats'].get('review_count', 0)}æ¡)"
            elif paper.get('reviews'):
                quality_source = "review"
            else:
                quality_source = "é»˜è®¤"
            title = paper.get('title', 'N/A')
            print(f"  - {paper_id} (ç›¸ä¼¼åº¦={similarity:.3f}, è´¨é‡={quality:.3f} [{quality_source}])")
            print(f"    æ ‡é¢˜: {title}")

            # ä»å›¾è°±ä¸­æ‰¾åˆ°Paperä½¿ç”¨çš„Pattern
            if not self.G.has_node(paper_id):
                continue

            for successor in self.G.successors(paper_id):
                edge_data = self.G[paper_id][successor]
                if edge_data.get('relation') == 'uses_pattern':
                    pattern_id = successor
                    pattern_quality = edge_data.get('quality', 0.5)

                    # å¾—åˆ† = Paperç›¸ä¼¼åº¦ Ã— Paperè´¨é‡ Ã— Patternè´¨é‡
                    score = combined_weight * pattern_quality
                    pattern_scores[pattern_id] += score

        # æ’åºå¹¶åªä¿ç•™Top-Kä¸ªPattern
        sorted_patterns = sorted(pattern_scores.items(), key=lambda x: x[1], reverse=True)
        top_patterns = dict(sorted_patterns[:RecallConfig.PATH3_FINAL_TOP_K])

        print(f"  âœ“ å¬å› {len(pattern_scores)} ä¸ªPatternï¼Œä¿ç•™Top-{RecallConfig.PATH3_FINAL_TOP_K}")
        return top_patterns

    # ===================== å®¡è®¡å·¥å…· =====================

    def _truncate(self, text: str, n: int) -> str:
        if not text:
            return ""
        if len(text) <= n:
            return text
        return text[:n] + "â€¦"

    def _topn_dict(self, d: Dict[str, float], n: int, key_name: str = "pattern_id") -> List[Dict]:
        ranked = sorted(d.items(), key=lambda x: x[1], reverse=True)[:n]
        return [{key_name: k, "score": v} for k, v in ranked]

    # ===================== å¤šè·¯èåˆ =====================

    def recall(self, user_idea: str, verbose: bool = True) -> List[Tuple[str, Dict, float]]:
        """ä¸‰è·¯å¬å›èåˆ

        Args:
            user_idea: ç”¨æˆ·è¾“å…¥çš„Ideaæè¿°
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯

        Returns:
            [(pattern_id, pattern_info, score), ...] æŒ‰å¾—åˆ†æ’åº
        """
        print("=" * 80)
        print("ğŸ¯ å¼€å§‹ä¸‰è·¯å¬å›")
        print("=" * 80)
        print(f"\nã€ç”¨æˆ·Ideaã€‘\n{user_idea}\n")
        if self.logger:
            self.logger.log_event("recall_start", {"user_idea": user_idea})

        # è·¯å¾„1: ç›¸ä¼¼Ideaå¬å›
        path1_scores, top_ideas = self._recall_path1_similar_ideas(user_idea)

        # è·¯å¾„2: é¢†åŸŸç›¸å…³æ€§å¬å›ï¼ˆä½¿ç”¨è·¯å¾„1çš„Top Ideasï¼‰
        path2_scores = self._recall_path2_domain_patterns(user_idea, top_ideas=top_ideas)

        # è·¯å¾„3: ç›¸ä¼¼Paperå¬å›
        path3_scores = self._recall_path3_similar_papers(user_idea)

        # èåˆä¸‰è·¯å¾—åˆ†
        print("\nğŸ”— èåˆä¸‰è·¯å¬å›ç»“æœ...")
        all_patterns = set(path1_scores.keys()) | set(path2_scores.keys()) | set(path3_scores.keys())

        final_scores = {}
        for pattern_id in all_patterns:
            score1 = path1_scores.get(pattern_id, 0.0) * RecallConfig.PATH1_WEIGHT
            score2 = path2_scores.get(pattern_id, 0.0) * RecallConfig.PATH2_WEIGHT
            score3 = path3_scores.get(pattern_id, 0.0) * RecallConfig.PATH3_WEIGHT

            final_scores[pattern_id] = score1 + score2 + score3

        # æ’åºå¹¶è¿”å›Top-K
        ranked = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)
        top_k = ranked[:RecallConfig.FINAL_TOP_K]

        # æ„å»ºè¿”å›ç»“æœ
        results = []
        for pattern_id, score in top_k:
            pattern_info = self.pattern_id_to_pattern.get(pattern_id, {})
            results.append((pattern_id, pattern_info, score))

        # æ‰“å°ç»“æœ
        if verbose:
            self._print_results(results, path1_scores, path2_scores, path3_scores)

        # å¬å›å®¡è®¡ï¼ˆå¯é€‰ï¼‰
        if PipelineConfig.RECALL_AUDIT_ENABLE:
            topn = max(0, int(PipelineConfig.RECALL_AUDIT_TOPN))
            snippet_len = max(0, int(PipelineConfig.RECALL_AUDIT_SNIPPET_CHARS))

            # è·¯å¾„1 top ideas
            top_ideas_audit = []
            for idea_id, sim in top_ideas[:RecallConfig.PATH1_TOP_K_IDEAS]:
                idea = self.idea_id_to_idea.get(idea_id, {})
                desc = idea.get("description", "")
                pattern_ids = idea.get("pattern_ids", []) or []
                top_ideas_audit.append({
                    "idea_id": idea_id,
                    "similarity": float(sim),
                    "snippet": self._truncate(desc, snippet_len),
                    "pattern_count": len(pattern_ids),
                })

            # è·¯å¾„2 top domains
            top_domains_raw = getattr(self, "_last_path2_top_domains", []) or []
            top_domains_audit = []
            for domain_id, weight in top_domains_raw[:RecallConfig.PATH2_TOP_K_DOMAINS]:
                domain = self.domain_id_to_domain.get(domain_id, {})
                top_domains_audit.append({
                    "domain_id": domain_id,
                    "name": domain.get("name", ""),
                    "weight": float(weight),
                    "paper_count": int(domain.get("paper_count", 0) or 0),
                })

            # è·¯å¾„3 top papers
            top_papers_raw = getattr(self, "_last_path3_top_papers", []) or []
            top_papers_audit = []
            for paper_id, sim, quality, _combined in top_papers_raw[:RecallConfig.PATH3_TOP_K_PAPERS]:
                paper = self.paper_id_to_paper.get(paper_id, {})
                review_stats = paper.get("review_stats") or {}
                top_papers_audit.append({
                    "paper_id": paper_id,
                    "similarity": float(sim),
                    "title": paper.get("title", ""),
                    "quality": float(quality),
                    "review_count": int(review_stats.get("review_count", 0) or 0),
                })

            # è®°å½•å„è·¯ score çš„ Top-Nï¼ˆåŠ æƒåçš„åˆ†æ•°ï¼‰
            path1_weighted = {k: v * RecallConfig.PATH1_WEIGHT for k, v in path1_scores.items()}
            path2_weighted = {k: v * RecallConfig.PATH2_WEIGHT for k, v in path2_scores.items()}
            path3_weighted = {k: v * RecallConfig.PATH3_WEIGHT for k, v in path3_scores.items()}

            final_top_k_audit = []
            for pattern_id, final_score in top_k:
                pattern_info = self.pattern_id_to_pattern.get(pattern_id, {})
                final_top_k_audit.append({
                    "pattern_id": pattern_id,
                    "name": pattern_info.get("name", "N/A"),
                    "final_score": float(final_score),
                    "path1_score": float(path1_weighted.get(pattern_id, 0.0)),
                    "path2_score": float(path2_weighted.get(pattern_id, 0.0)),
                    "path3_score": float(path3_weighted.get(pattern_id, 0.0)),
                    "cluster_size": int(pattern_info.get("size", 0) or 0),
                })

            self.last_audit = {
                "final_top_k": final_top_k_audit,
                "path1": {
                    "top_ideas": top_ideas_audit,
                    "pattern_scores_topn": self._topn_dict(path1_weighted, topn),
                },
                "path2": {
                    "top_domains": top_domains_audit,
                    "pattern_scores_topn": self._topn_dict(path2_weighted, topn),
                },
                "path3": {
                    "top_papers": top_papers_audit,
                    "pattern_scores_topn": self._topn_dict(path3_weighted, topn),
                },
            }
        else:
            self.last_audit = None

        if self.logger:
            self.logger.log_event("recall_end", {"top_k": len(results)})

        return results

    def _print_results(self, results: List[Tuple[str, Dict, float]],
                      path1_scores: Dict, path2_scores: Dict, path3_scores: Dict):
        """æ‰“å°å¬å›ç»“æœ"""
        print("\n" + "=" * 80)
        print(f"ğŸ“Š å¬å›ç»“æœ Top-{RecallConfig.FINAL_TOP_K}")
        print("=" * 80)

        for rank, (pattern_id, pattern_info, final_score) in enumerate(results, 1):
            print(f"\nã€Rank {rank}ã€‘ {pattern_id}")
            print(f"  åç§°: {pattern_info.get('name', 'N/A')}")
            print(f"  æœ€ç»ˆå¾—åˆ†: {final_score:.4f}")

            # æ˜¾ç¤ºå„è·¯å¾—åˆ†
            score1 = path1_scores.get(pattern_id, 0.0) * RecallConfig.PATH1_WEIGHT
            score2 = path2_scores.get(pattern_id, 0.0) * RecallConfig.PATH2_WEIGHT
            score3 = path3_scores.get(pattern_id, 0.0) * RecallConfig.PATH3_WEIGHT

            print(f"  - è·¯å¾„1 (ç›¸ä¼¼Idea):   {score1:.4f} (å æ¯” {score1/final_score*100:.1f}%)")
            print(f"  - è·¯å¾„2 (é¢†åŸŸç›¸å…³):   {score2:.4f} (å æ¯” {score2/final_score*100:.1f}%)")
            print(f"  - è·¯å¾„3 (ç›¸ä¼¼Paper):  {score3:.4f} (å æ¯” {score3/final_score*100:.1f}%)")

            print(f"  èšç±»å¤§å°: {pattern_info.get('size', 0)} ç¯‡è®ºæ–‡")

            # V3ç‰ˆæœ¬: ä¼˜å…ˆæ˜¾ç¤ºLLMå¢å¼ºçš„æ€»ç»“ï¼Œå¦åˆ™æ˜¾ç¤ºåŸå§‹ç¤ºä¾‹
            if pattern_info.get('llm_enhanced_summary'):
                llm_summary = pattern_info['llm_enhanced_summary'].get('representative_ideas', '')
                print(f"  å½’çº³æ€»ç»“: {llm_summary[:120]}...")
            else:
                summary = pattern_info.get('summary', {})
                ideas = summary.get('representative_ideas', [])
                if ideas:
                    print(f"  ç¤ºä¾‹Idea: {ideas[0][:120] if ideas else 'N/A'}...")

        print("\n" + "=" * 80)


# ===================== Demo æµ‹è¯•ç”¨ä¾‹ =====================
def demo():
    """è¿è¡ŒDemo"""

    # åˆå§‹åŒ–å¬å›ç³»ç»Ÿ
    system = RecallSystem()

    # æµ‹è¯•ç”¨ä¾‹
    test_ideas = [
        "ä½¿ç”¨Transformeræ¨¡å‹è¿›è¡Œæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸ŠéªŒè¯æ•ˆæœ",
        "æå‡ºä¸€ç§æ–°çš„æ³¨æ„åŠ›æœºåˆ¶æ”¹è¿›ç¥ç»æœºå™¨ç¿»è¯‘çš„å¯¹é½è´¨é‡",
        "é€šè¿‡å¯¹æŠ—è®­ç»ƒæå‡æ¨¡å‹åœ¨å¯¹è¯ç³»ç»Ÿä¸­çš„é²æ£’æ€§",
        "åˆ©ç”¨çŸ¥è¯†å›¾è°±å¢å¼ºé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„è¯­ä¹‰ç†è§£èƒ½åŠ›",
    ]

    for i, user_idea in enumerate(test_ideas, 1):
        print("\n\n")
        print("ğŸ¬" * 40)
        print(f"æµ‹è¯•ç”¨ä¾‹ {i}/{len(test_ideas)}")
        print("ğŸ¬" * 40)

        results = system.recall(user_idea, verbose=True)

        # ç­‰å¾…ç”¨æˆ·æŸ¥çœ‹ç»“æœ
        if i < len(test_ideas):
            input("\næŒ‰Enterç»§ç»­ä¸‹ä¸€ä¸ªæµ‹è¯•ç”¨ä¾‹...")


if __name__ == '__main__':
    demo()
