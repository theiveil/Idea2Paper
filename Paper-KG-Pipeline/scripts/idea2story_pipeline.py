"""
Idea2Story Pipeline - ä»ç”¨æˆ· Idea åˆ°å¯å‘è¡¨çš„ Paper Story

å®ç°æµç¨‹:
  Phase 1: Pattern Selection (ç­–ç•¥é€‰æ‹©)
  Phase 2: Story Generation (ç»“æ„åŒ–ç”Ÿæˆ)
  Phase 3: Multi-Agent Critic & Refine (è¯„å®¡ä¸ä¿®æ­£)
  Phase 4: RAG Verification & Pivot (æŸ¥é‡ä¸è§„é¿)

ä½¿ç”¨æ–¹æ³•:
  python scripts/idea2story_pipeline.py "ä½ çš„Ideaæè¿°"
"""

import json
import os
import sys
import time
import uuid
from datetime import datetime, timezone
from pathlib import Path

# æå‰åŠ è½½ .envï¼ˆç¡®ä¿ PipelineConfig è¯»å–å‰ç”Ÿæ•ˆï¼‰
SCRIPT_DIR = Path(__file__).resolve().parent
PROJECT_ROOT = SCRIPT_DIR.parent
REPO_ROOT = PROJECT_ROOT.parent
SRC_DIR = PROJECT_ROOT / "src"
if str(SRC_DIR) not in sys.path:
    sys.path.insert(0, str(SRC_DIR))

try:
    from idea2paper.infra.dotenv import load_dotenv
    _DOTENV_STATUS = load_dotenv(REPO_ROOT / ".env", override=False)
except Exception as _e:
    _DOTENV_STATUS = {"loaded": 0, "path": str(REPO_ROOT / ".env"), "ok": False, "error": str(_e)}

# å¯¼å…¥ Pipeline æ¨¡å—
try:
    from pipeline import Idea2StoryPipeline, OUTPUT_DIR
    from pipeline.config import (
        LOG_ROOT,
        ENABLE_RUN_LOGGING,
        LOG_MAX_TEXT_CHARS,
        REPO_ROOT,
        RESULTS_ROOT,
        RESULTS_ENABLE,
        RESULTS_MODE,
        RESULTS_KEEP_LOG,
        NOVELTY_ENABLE,
        NOVELTY_INDEX_DIR,
        NOVELTY_INDEX_BUILD_BATCH_SIZE,
        NOVELTY_INDEX_BUILD_RESUME,
        NOVELTY_INDEX_BUILD_MAX_RETRIES,
        NOVELTY_INDEX_BUILD_SLEEP_SEC,
        NOVELTY_REQUIRE_EMBEDDING,
    )
    from pipeline.config import PipelineConfig
    from idea2paper.infra.result_bundler import ResultBundler
    from idea2paper.infra.index_preflight import (
        validate_novelty_index,
        validate_recall_index,
        acquire_lock,
    )
    from idea2paper.infra.embeddings import EMBEDDING_MODEL
    from pipeline.run_logger import RunLogger
    from pipeline.run_context import set_logger, reset_logger
    from tools.build_novelty_index import build_novelty_index
    from tools.build_recall_index import build_recall_index
except ImportError:
    # å¦‚æœç›´æ¥è¿è¡Œè„šæœ¬ï¼Œå°è¯•æ·»åŠ å½“å‰ç›®å½•åˆ° path
    import os
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
    from pipeline import Idea2StoryPipeline, OUTPUT_DIR
    from pipeline.config import (
        LOG_ROOT,
        ENABLE_RUN_LOGGING,
        LOG_MAX_TEXT_CHARS,
        REPO_ROOT,
        RESULTS_ROOT,
        RESULTS_ENABLE,
        RESULTS_MODE,
        RESULTS_KEEP_LOG,
        NOVELTY_ENABLE,
        NOVELTY_INDEX_DIR,
        NOVELTY_INDEX_BUILD_BATCH_SIZE,
        NOVELTY_INDEX_BUILD_RESUME,
        NOVELTY_INDEX_BUILD_MAX_RETRIES,
        NOVELTY_INDEX_BUILD_SLEEP_SEC,
        NOVELTY_REQUIRE_EMBEDDING,
    )
    from pipeline.config import PipelineConfig
    from idea2paper.infra.result_bundler import ResultBundler
    from idea2paper.infra.index_preflight import (
        validate_novelty_index,
        validate_recall_index,
        acquire_lock,
    )
    from idea2paper.infra.embeddings import EMBEDDING_MODEL
    from pipeline.run_logger import RunLogger
    from pipeline.run_context import set_logger, reset_logger
    from tools.build_novelty_index import build_novelty_index
    from tools.build_recall_index import build_recall_index


def _log_event(logger, event_type: str, payload: dict):
    if logger:
        logger.log_event(event_type, payload)


def ensure_required_indexes(logger=None):
    if not PipelineConfig.INDEX_AUTO_PREPARE:
        return

    _log_event(logger, "index_preflight_start", {
        "novelty_enable": NOVELTY_ENABLE,
        "recall_use_offline_index": PipelineConfig.RECALL_USE_OFFLINE_INDEX,
        "allow_build": PipelineConfig.INDEX_ALLOW_BUILD,
    })

    # Novelty index preflight
    if NOVELTY_ENABLE:
        nodes_paper_path = OUTPUT_DIR / "nodes_paper.json"
        status = validate_novelty_index(NOVELTY_INDEX_DIR, nodes_paper_path, EMBEDDING_MODEL)
        if status.get("ok"):
            _log_event(logger, "index_preflight_ok", {"index": "novelty", "status": status})
        else:
            _log_event(logger, "index_preflight_failed", {"index": "novelty", "status": status})
            if PipelineConfig.INDEX_ALLOW_BUILD:
                lock_path = NOVELTY_INDEX_DIR / ".build.lock"
                _log_event(logger, "index_preflight_build_start", {
                    "index": "novelty",
                    "index_dir": str(NOVELTY_INDEX_DIR),
                })
                with acquire_lock(lock_path):
                    build_novelty_index(
                        index_dir=NOVELTY_INDEX_DIR,
                        batch_size=NOVELTY_INDEX_BUILD_BATCH_SIZE,
                        resume=NOVELTY_INDEX_BUILD_RESUME,
                        max_retries=NOVELTY_INDEX_BUILD_MAX_RETRIES,
                        sleep_sec=NOVELTY_INDEX_BUILD_SLEEP_SEC,
                        force_rebuild=False,
                        logger=logger,
                    )
                status = validate_novelty_index(NOVELTY_INDEX_DIR, nodes_paper_path, EMBEDDING_MODEL)
                _log_event(logger, "index_preflight_build_done", {"index": "novelty", "status": status})
                if not status.get("ok") and NOVELTY_REQUIRE_EMBEDDING:
                    raise RuntimeError("Novelty index build failed or incomplete. Please run build_novelty_index.py manually.")
            else:
                if NOVELTY_REQUIRE_EMBEDDING:
                    raise RuntimeError(
                        "Novelty index missing or mismatched. Please run: "
                        "python Paper-KG-Pipeline/scripts/tools/build_novelty_index.py --resume"
                    )
                print("âš ï¸ Novelty index missing/mismatch. Continuing because require_embedding=false.")

    # Recall offline index (only if enabled)
    if PipelineConfig.RECALL_USE_OFFLINE_INDEX:
        nodes_paper_path = OUTPUT_DIR / "nodes_paper.json"
        nodes_idea_path = OUTPUT_DIR / "nodes_idea.json"
        status = validate_recall_index(PipelineConfig.RECALL_INDEX_DIR, nodes_paper_path, nodes_idea_path, EMBEDDING_MODEL)
        if status.get("ok"):
            _log_event(logger, "index_preflight_ok", {"index": "recall", "status": status})
        else:
            _log_event(logger, "index_preflight_failed", {"index": "recall", "status": status})
            if PipelineConfig.INDEX_ALLOW_BUILD:
                lock_path = Path(PipelineConfig.RECALL_INDEX_DIR) / ".build.lock"
                _log_event(logger, "index_preflight_build_start", {
                    "index": "recall",
                    "index_dir": str(PipelineConfig.RECALL_INDEX_DIR),
                })
                with acquire_lock(lock_path):
                    build_recall_index(
                        index_dir=PipelineConfig.RECALL_INDEX_DIR,
                        batch_size=PipelineConfig.RECALL_EMBED_BATCH_SIZE,
                        resume=True,
                        max_retries=PipelineConfig.RECALL_EMBED_MAX_RETRIES,
                        sleep_sec=PipelineConfig.RECALL_EMBED_SLEEP_SEC,
                        force_rebuild=False,
                        logger=logger,
                    )
                status = validate_recall_index(PipelineConfig.RECALL_INDEX_DIR, nodes_paper_path, nodes_idea_path, EMBEDDING_MODEL)
                _log_event(logger, "index_preflight_build_done", {"index": "recall", "status": status})
            else:
                print("âš ï¸ Recall offline index missing/mismatch. Continuing with online batch fallback.")

# ===================== ä¸»å‡½æ•° =====================
def main():
    """ä¸»å‡½æ•°"""
    # è·å–ç”¨æˆ·è¾“å…¥
    if len(sys.argv) > 1:
        user_idea = " ".join(sys.argv[1:])
    else:
        user_idea = "LLM-Assisted Domain Data Extraction and Cleaning"

    # åŠ è½½å¬å›ç»“æœï¼ˆè°ƒç”¨ simple_recall_demo çš„ç»“æœï¼‰
    print("ğŸ“‚ åŠ è½½æ•°æ®...")

    logger = None
    token = None
    start_time = time.time()
    start_dt = datetime.now(timezone.utc)
    run_id = f"run_{start_dt.strftime('%Y%m%d_%H%M%S')}_{os.getpid()}_{uuid.uuid4().hex[:6]}"
    success = False

    try:
        if ENABLE_RUN_LOGGING:
            logger = RunLogger(
                base_dir=LOG_ROOT,
                run_id=run_id,
                meta={
                    "user_idea": user_idea,
                    "argv": sys.argv,
                    "entrypoint": __file__,
                },
                max_text_chars=LOG_MAX_TEXT_CHARS
            )
            token = set_logger(logger)
            logger.log_event("run_start", {"user_idea": user_idea})
            if _DOTENV_STATUS:
                logger.log_event("dotenv_loaded", _DOTENV_STATUS)
        # Preflight & auto-prepare required indexes (quality-first)
        ensure_required_indexes(logger)
        # åŠ è½½èŠ‚ç‚¹æ•°æ®
        with open(OUTPUT_DIR / "nodes_pattern.json", 'r', encoding='utf-8') as f:
            patterns = json.load(f)
        with open(OUTPUT_DIR / "nodes_paper.json", 'r', encoding='utf-8') as f:
            papers = json.load(f)

        print(f"  âœ“ åŠ è½½ {len(patterns)} ä¸ª Pattern")
        print(f"  âœ“ åŠ è½½ {len(papers)} ä¸ª Paper")

        # è¿è¡Œå¬å›ï¼ˆå¤ç”¨ simple_recall_demo çš„é€»è¾‘ï¼‰
        # æ³¨æ„ï¼šè¿™é‡Œä¸ºäº†å¤ç”¨é€»è¾‘ï¼Œç›´æ¥å¯¼å…¥äº† simple_recall_demo
        # åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œå»ºè®®å°†å¬å›é€»è¾‘å°è£…ä¸ºç‹¬ç«‹çš„ç±»

        # ä¸´æ—¶ä¿å­˜åŸå§‹ argv
        original_argv = sys.argv.copy()
        sys.argv = ['simple_recall_demo.py', user_idea]

        # è¿è¡Œå¬å›ï¼ˆä½¿ç”¨ RecallSystem ç±»ï¼Œæ”¯æŒä¸¤é˜¶æ®µä¼˜åŒ–ï¼‰
        print("\nğŸ” è¿è¡Œå¬å›ç³»ç»Ÿ...")
        print("-" * 80)

        # ã€ä¼˜åŒ–ã€‘ç›´æ¥ä½¿ç”¨ RecallSystem ç±»ï¼ˆæ”¯æŒä¸¤é˜¶æ®µå¬å›ï¼Œå¤§å¹…æé€Ÿï¼‰
        from recall_system import RecallSystem

        print("  åˆå§‹åŒ–å¬å›ç³»ç»Ÿ...")
        recall_system = RecallSystem()

        print("\n  æ‰§è¡Œä¸‰è·¯å¬å›ï¼ˆä¼˜åŒ–ç‰ˆï¼Œæ”¯æŒä¸¤é˜¶æ®µåŠ é€Ÿï¼‰...")
        recall_results = recall_system.recall(user_idea, verbose=True)
        recall_audit = getattr(recall_system, "last_audit", None)

        # ã€å…³é”®ä¿®å¤ã€‘åŠ è½½å®Œæ•´çš„ patterns_structured.json ä»¥åˆå¹¶æ•°æ®
        patterns_structured_file = OUTPUT_DIR / "patterns_structured.json"
        if patterns_structured_file.exists():
            with open(patterns_structured_file, 'r', encoding='utf-8') as f:
                patterns_structured = json.load(f)

            # æ„å»º pattern_id -> structured_data çš„æ˜ å°„
            structured_map = {}
            for p in patterns_structured:
                pattern_id = f"pattern_{p.get('pattern_id')}"
                structured_map[pattern_id] = p

            # åˆå¹¶ skeleton_examples å’Œ common_tricks åˆ°å¬å›ç»“æœ
            merged_results = []
            for pattern_id, pattern_info, score in recall_results:
                merged_pattern = dict(pattern_info)
                if pattern_id in structured_map:
                    merged_pattern['skeleton_examples'] = structured_map[pattern_id].get('skeleton_examples', [])
                    merged_pattern['common_tricks'] = structured_map[pattern_id].get('common_tricks', [])
                merged_results.append((pattern_id, merged_pattern, score))

            recalled_patterns = merged_results
        else:
            # å¦‚æœæ²¡æœ‰ patterns_structured.jsonï¼Œç›´æ¥ä½¿ç”¨å¬å›ç»“æœ
            recalled_patterns = recall_results

        # åŠ è½½ papers æ•°æ® (Pipeline éœ€è¦ç”¨äº RAG æŸ¥é‡)
        print("\n  åŠ è½½ Papers æ•°æ®ç”¨äºæŸ¥é‡...")
        with open(OUTPUT_DIR / "nodes_paper.json", 'r', encoding='utf-8') as f:
            papers = json.load(f)

        # æ¢å¤ argv
        sys.argv = original_argv

        print("-" * 80)
        print(f"âœ… å¬å›å®Œæˆ: Top-{len(recalled_patterns)} Patterns\n")

        # è¿è¡Œ Pipelineï¼ˆä¼ é€’ user_idea ç”¨äº Pattern æ™ºèƒ½åˆ†ç±»ï¼‰
        pipeline = Idea2StoryPipeline(user_idea, recalled_patterns, papers, run_id=run_id)
        result = pipeline.run()
        if recall_audit is not None:
            result["recall_audit"] = recall_audit
            if logger and PipelineConfig.RECALL_AUDIT_IN_EVENTS:
                logger.log_event("recall_audit", recall_audit)
        success = True

        # ä¿å­˜ç»“æœ
        output_file = OUTPUT_DIR / "final_story.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result['final_story'], f, ensure_ascii=False, indent=2)

        print(f"\nğŸ’¾ æœ€ç»ˆ Story å·²ä¿å­˜åˆ°: {output_file}")

        # ä¿å­˜å®Œæ•´ç»“æœ
        full_result_file = OUTPUT_DIR / "pipeline_result.json"
        results_dir = str(RESULTS_ROOT / run_id) if RESULTS_ENABLE else None
        with open(full_result_file, 'w', encoding='utf-8') as f:
            json.dump({
                'user_idea': user_idea,
                'success': result['success'],
                'iterations': result['iterations'],
                'selected_patterns': result['selected_patterns'],
                'final_story': result['final_story'],
                'review_history': result['review_history'],
                'results_dir': results_dir,
                'novelty_report': result.get('novelty_report'),
                'recall_audit': result.get('recall_audit'),
                'review_summary': {
                    'total_reviews': len(result['review_history']),
                    'final_score': result['review_history'][-1]['avg_score'] if result['review_history'] else 0
                },
                'refinement_summary': {
                    'total_refinements': len(result['refinement_history']),
                    'issues_addressed': [r['issue'] for r in result['refinement_history']]
                },
                'verification_summary': {
                    'collision_detected': result['verification_result']['collision_detected'],
                    'max_similarity': result['verification_result']['max_similarity']
                }
            }, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ å®Œæ•´ç»“æœå·²ä¿å­˜åˆ°: {full_result_file}")

        # èšåˆäº§ç‰©åˆ° repo æ ¹ results/
        if RESULTS_ENABLE:
            try:
                bundler = ResultBundler(
                    repo_root=REPO_ROOT,
                    results_root=RESULTS_ROOT,
                    mode=RESULTS_MODE,
                    keep_log=RESULTS_KEEP_LOG,
                )
                run_log_dir = (LOG_ROOT / run_id) if ENABLE_RUN_LOGGING else None
                novelty_report_path = None
                if isinstance(result.get("novelty_report"), dict):
                    novelty_report_path = result["novelty_report"].get("report_path")
                bundle_status = bundler.bundle(
                    run_id=run_id,
                    user_idea=user_idea,
                    success=success,
                    output_dir=OUTPUT_DIR,
                    run_log_dir=run_log_dir,
                    extra={
                        "config_snapshot": {
                            "results": {
                                "enable": RESULTS_ENABLE,
                                "dir": str(RESULTS_ROOT),
                                "mode": RESULTS_MODE,
                                "keep_log": RESULTS_KEEP_LOG,
                            },
                            "logging": {
                                "enable": ENABLE_RUN_LOGGING,
                                "dir": str(LOG_ROOT),
                                "max_text_chars": LOG_MAX_TEXT_CHARS,
                            },
                            "critic": {
                                "strict_json": PipelineConfig.CRITIC_STRICT_JSON,
                                "json_retries": PipelineConfig.CRITIC_JSON_RETRIES,
                            },
                            "pass": {
                                "mode": PipelineConfig.PASS_MODE,
                                "min_pattern_papers": PipelineConfig.PASS_MIN_PATTERN_PAPERS,
                                "fallback": PipelineConfig.PASS_FALLBACK,
                                "fixed_score": PipelineConfig.PASS_SCORE,
                            },
                        },
                        "novelty_report_path": novelty_report_path
                    },
                )
                if bundle_status.get("ok"):
                    print(f"âœ… Results bundled to: {bundle_status.get('results_dir')}")
                    if logger:
                        logger.log_event("results_bundled", {
                            "results_dir": bundle_status.get("results_dir"),
                            "mode": RESULTS_MODE,
                            "partial": bundle_status.get("partial", False)
                        })
                else:
                    if logger:
                        logger.log_event("results_bundle_failed", {
                            "errors": bundle_status.get("errors", []),
                            "mode": RESULTS_MODE
                        })
            except Exception as e:
                print(f"[results] warning: bundling failed: {e}")
                if logger:
                    logger.log_event("results_bundle_failed", {"error": str(e)})

    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        if logger:
            logger.log_event("run_error", {"error": str(e)})
        import traceback
        traceback.print_exc()
    finally:
        if logger:
            logger.log_event("run_end", {
                "success": success,
                "duration_ms": int((time.time() - start_time) * 1000)
            })
        if token is not None:
            reset_logger(token)


if __name__ == '__main__':
    main()
