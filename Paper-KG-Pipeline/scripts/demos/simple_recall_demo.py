"""
ç®€åŒ–çš„å¬å›ç³»ç»ŸDemo - å•ä¸ªæµ‹è¯•ç”¨ä¾‹ (V3ç‰ˆæœ¬)

ä½¿ç”¨æ–¹æ³•:
  python scripts/simple_recall_demo.py "ä½ çš„Ideaæè¿°"

ç¤ºä¾‹:
  python scripts/simple_recall_demo.py "ä½¿ç”¨Transformerè¿›è¡Œæ–‡æœ¬åˆ†ç±»"

V3ç‰ˆæœ¬æ›´æ–°:
  - é€‚é…V3èŠ‚ç‚¹ç»“æ„ (Paper.ideaä¸ºå­—ç¬¦ä¸²ï¼ŒéåµŒå¥—å­—å…¸)
  - è·¯å¾„1ç›´æ¥ä½¿ç”¨Idea.pattern_idsï¼Œæ— éœ€é€šè¿‡Paperä¸­è½¬
  - Paperé€šè¿‡review_statsè·å–è´¨é‡åˆ†æ•°ï¼Œæ”¯æŒå…¼å®¹æ—§ç»“æ„
"""

import json
import os
import pickle
import sys
import time
from collections import defaultdict
from pathlib import Path

from tqdm import tqdm

# æå‰åŠ è½½ .envï¼ˆç¡®ä¿é…ç½®è¯»å–å‰ç”Ÿæ•ˆï¼‰
SCRIPT_DIR = Path(__file__).resolve().parent
SCRIPTS_DIR = SCRIPT_DIR.parent
PROJECT_ROOT = SCRIPTS_DIR.parent
REPO_ROOT = PROJECT_ROOT.parent
SRC_DIR = PROJECT_ROOT / "src"
if str(SRC_DIR) not in sys.path:
    sys.path.insert(0, str(SRC_DIR))
if str(SCRIPTS_DIR) not in sys.path:
    sys.path.insert(0, str(SCRIPTS_DIR))

try:
    from idea2paper.infra.dotenv import load_dotenv
    _DOTENV_STATUS = load_dotenv(REPO_ROOT / ".env", override=False)
except Exception:
    _DOTENV_STATUS = None

import numpy as np
import requests

from pipeline.run_context import get_logger
# ===================== è·¯å¾„é…ç½® =====================
OUTPUT_DIR = PROJECT_ROOT / "output"

NODES_IDEA = OUTPUT_DIR / "nodes_idea.json"
NODES_PATTERN = OUTPUT_DIR / "nodes_pattern.json"
NODES_DOMAIN = OUTPUT_DIR / "nodes_domain.json"
NODES_PAPER = OUTPUT_DIR / "nodes_paper.json"
GRAPH_FILE = OUTPUT_DIR / "knowledge_graph_v2.gpickle"

# ===================== é…ç½®å‚æ•° =====================
TOP_K_IDEAS = 10
TOP_K_PATTERNS_PATH1 = 10  # è·¯å¾„1æœ€ç»ˆä¿ç•™Top-Kä¸ªPatternï¼ˆé‡è¦é€šé“ï¼‰

TOP_K_DOMAINS = 5
TOP_K_PATTERNS_PATH2 = 5   # è·¯å¾„2æœ€ç»ˆä¿ç•™Top-Kä¸ªPatternï¼ˆè¾…åŠ©é€šé“ï¼‰

TOP_K_PAPERS = 20
TOP_K_PATTERNS_PATH3 = 10  # è·¯å¾„3æœ€ç»ˆä¿ç•™Top-Kä¸ªPatternï¼ˆé‡è¦é€šé“ï¼‰

FINAL_TOP_K = 10

PATH1_WEIGHT = 0.4  # ç›¸ä¼¼Idea - é‡è¦
PATH2_WEIGHT = 0.2  # é¢†åŸŸç›¸å…³ - è¾…åŠ©
PATH3_WEIGHT = 0.4  # ç›¸ä¼¼Paper - é‡è¦

USE_EMBEDDING = True  # ä½¿ç”¨embeddingè®¡ç®—ç›¸ä¼¼åº¦ï¼ˆæ¨èï¼‰

# ä¸¤é˜¶æ®µå¬å›ä¼˜åŒ–ï¼ˆç²—æ’+ç²¾æ’ï¼‰
TWO_STAGE_RECALL = True      # å¯ç”¨ä¸¤é˜¶æ®µå¬å›ï¼ˆå¤§å¹…æé€Ÿï¼‰
COARSE_RECALL_SIZE = 100     # ç²—å¬å›æ•°é‡ï¼ˆJaccardå¿«é€Ÿç­›é€‰ï¼‰


# ===================== å·¥å…·å‡½æ•° =====================
def compute_similarity(text1, text2):
    """è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦"""
    if USE_EMBEDDING:
        return compute_embedding_similarity(text1, text2)
    else:
        return compute_jaccard_similarity(text1, text2)

def compute_jaccard_similarity(text1, text2):
    """è¯è¢‹Jaccardç›¸ä¼¼åº¦ï¼ˆå¿«é€Ÿä½†ä¸å‡†ç¡®ï¼‰"""
    tokens1 = set(text1.lower().split())
    tokens2 = set(text2.lower().split())

    if not tokens1 or not tokens2:
        return 0.0

    intersection = len(tokens1 & tokens2)
    union = len(tokens1 | tokens2)
    return intersection / union

def compute_embedding_similarity(text1, text2):
    """åŸºäºembeddingçš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆæ›´å‡†ç¡®ï¼‰"""
    emb1 = get_embedding(text1)
    emb2 = get_embedding(text2)

    if emb1 is None or emb2 is None:
        return compute_jaccard_similarity(text1, text2)

    emb1 = np.array(emb1)
    emb2 = np.array(emb2)

    cosine_sim = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))
    return float(cosine_sim)

_embedding_cache = {}
_embedding_warning_shown = False
_embedding_error_shown = False

def get_embedding(text, max_retries=3):
    """è°ƒç”¨SiliconFlow APIè·å–æ–‡æœ¬embedding"""
    global _embedding_warning_shown, _embedding_error_shown
    logger = get_logger()

    # ç¼“å­˜æ£€æŸ¥
    if text in _embedding_cache:
        return _embedding_cache[text]

    api_key = os.environ.get('SILICONFLOW_API_KEY', '')

    if not api_key:
        if not _embedding_warning_shown:
            print("  âš ï¸  æœªè®¾ç½®SILICONFLOW_API_KEYï¼Œé™çº§åˆ°Jaccardç›¸ä¼¼åº¦")
            _embedding_warning_shown = True
        return None

    url = "https://api.siliconflow.cn/v1/embeddings"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }

    payload = {
        "model": "Qwen/Qwen3-Embedding-8B",
        "input": text[:2000]
    }

    for attempt in range(max_retries):
        try:
            start_ts = time.time()
            response = requests.post(url, headers=headers, json=payload, timeout=10)
            response.raise_for_status()
            result = response.json()
            embedding = result['data'][0]['embedding']
            _embedding_cache[text] = embedding
            if logger:
                logger.log_embedding_call(
                    request={
                        "provider": "siliconflow",
                        "url": url,
                        "model": payload["model"],
                        "input_preview": text[:2000],
                        "timeout": 10
                    },
                    response={
                        "ok": True,
                        "latency_ms": int((time.time() - start_ts) * 1000)
                    }
                )
            return embedding
        except Exception as e:
            if attempt < max_retries - 1:
                time.sleep(0.5)
            else:
                if not _embedding_error_shown:
                    print(f"  âš ï¸  Embedding APIè°ƒç”¨å¤±è´¥: {e}ï¼Œé™çº§åˆ°Jaccardç›¸ä¼¼åº¦")
                    _embedding_error_shown = True
                if logger:
                    logger.log_embedding_call(
                        request={
                            "provider": "siliconflow",
                            "url": url,
                            "model": payload["model"],
                            "input_preview": text[:2000],
                            "timeout": 10
                        },
                        response={
                            "ok": False,
                            "latency_ms": 0,
                            "error": str(e)
                        }
                    )
                return None

    return None


def get_paper_quality(paper):
    """è®¡ç®—Paperçš„ç»¼åˆè´¨é‡åˆ†æ•°

    åŸºäºreviewçš„è¯„åˆ†ï¼Œå½’ä¸€åŒ–åˆ°[0, 1]
    å¦‚æœæ²¡æœ‰reviewæ•°æ®ï¼Œè¿”å›é»˜è®¤å€¼0.5
    """
    # ä¼˜å…ˆä½¿ç”¨æ–°ç»“æ„ä¸­çš„ review_stats.avg_score
    review_stats = paper.get('review_stats', {})

    if review_stats and review_stats.get('avg_score'):
        # å·²ç»æ˜¯ 0-1 çš„åˆ†æ•°
        return float(review_stats['avg_score'])

    # å¤‡é€‰æ–¹æ¡ˆï¼šå…¼å®¹æ—§ç»“æ„ï¼ˆreview åˆ—è¡¨ï¼‰
    reviews = paper.get('reviews', [])

    if not reviews:
        return 0.5  # é»˜è®¤ä¸­ç­‰è´¨é‡

    # æå–æ‰€æœ‰è¯„åˆ†
    scores = []
    for review in reviews:
        score_str = review.get('overall_score', '')
        # å°è¯•è§£æè¯„åˆ†ï¼ˆå¯èƒ½æ˜¯ "7", "7/10", "7.0" ç­‰æ ¼å¼ï¼‰
        try:
            if '/' in score_str:
                score_str = score_str.split('/')[0]
            score = float(score_str.strip())
            scores.append(score)
        except (ValueError, AttributeError):
            continue

    if not scores:
        return 0.5

    # è®¡ç®—å¹³å‡åˆ†å¹¶å½’ä¸€åŒ–
    import numpy as np
    avg_score = np.mean(scores)
    # å‡è®¾è¯„åˆ†èŒƒå›´æ˜¯ 1-10ï¼Œå½’ä¸€åŒ–åˆ° [0, 1]
    normalized_score = (avg_score - 1) / 9

    return min(max(normalized_score, 0.0), 1.0)


# ===================== ä¸»å‡½æ•° =====================
def main():
    # è·å–ç”¨æˆ·è¾“å…¥
    if len(sys.argv) > 1:
        user_idea = " ".join(sys.argv[1:])
    else:
        # user_idea = "ä½¿ç”¨è’¸é¦æŠ€æœ¯å®ŒæˆTransformerè·¨é¢†åŸŸæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸ŠéªŒè¯æ•ˆæœ"
        user_idea = "Research on the Self-Evolution of Intelligent Agents Based on Reflection and Memory"

    print("=" * 80)
    print("ğŸ¯ ä¸‰è·¯å¬å›ç³»ç»Ÿ Demo")
    print("=" * 80)
    print(f"\nã€ç”¨æˆ·Ideaã€‘\n{user_idea}\n")

    # åŠ è½½æ•°æ®
    print("ğŸ“‚ åŠ è½½æ•°æ®...")
    with open(NODES_IDEA, 'r', encoding='utf-8') as f:
        ideas = json.load(f)
    with open(NODES_PATTERN, 'r', encoding='utf-8') as f:
        patterns = json.load(f)
    with open(NODES_DOMAIN, 'r', encoding='utf-8') as f:
        domains = json.load(f)
    with open(NODES_PAPER, 'r', encoding='utf-8') as f:
        papers = json.load(f)
    with open(GRAPH_FILE, 'rb') as f:
        G = pickle.load(f)

    # æ„å»ºç´¢å¼•
    idea_map = {i['idea_id']: i for i in ideas}
    pattern_map = {p['pattern_id']: p for p in patterns}
    domain_map = {d['domain_id']: d for d in domains}
    paper_map = {p['paper_id']: p for p in papers}

    print(f"  âœ“ Idea: {len(ideas)}, Pattern: {len(patterns)}, Domain: {len(domains)}, Paper: {len(papers)}")
    print(f"  âœ“ å›¾è°±: {G.number_of_nodes()} èŠ‚ç‚¹, {G.number_of_edges()} è¾¹\n")

    # ===================== è·¯å¾„1: ç›¸ä¼¼Ideaå¬å› =====================
    print("ğŸ” [è·¯å¾„1] ç›¸ä¼¼Ideaå¬å›...")

    # ä¸¤é˜¶æ®µå¬å›ä¼˜åŒ–
    if TWO_STAGE_RECALL and USE_EMBEDDING:
        print(f"  [ç²—æ’] ä½¿ç”¨Jaccardå¿«é€Ÿç­›é€‰Top-{COARSE_RECALL_SIZE}...")
        coarse_similarities = []
        for idea in ideas:
            sim = compute_jaccard_similarity(user_idea, idea['description'])
            if sim > 0:
                coarse_similarities.append((idea['idea_id'], sim))

        coarse_similarities.sort(key=lambda x: x[1], reverse=True)
        candidates = coarse_similarities[:COARSE_RECALL_SIZE]

        print(f"  [ç²¾æ’] ä½¿ç”¨Embeddingé‡æ’Top-{TOP_K_IDEAS}...")
        fine_similarities = []
        for idea_id, _ in candidates:
            idea = idea_map[idea_id]
            sim = compute_embedding_similarity(user_idea, idea['description'])
            if sim > 0:
                fine_similarities.append((idea_id, sim))

        fine_similarities.sort(key=lambda x: x[1], reverse=True)
        top_ideas = fine_similarities[:TOP_K_IDEAS]

        print(f"  âœ“ ç²—æ’{len(coarse_similarities)}ä¸ª â†’ ç²¾æ’{len(candidates)}ä¸ª â†’ æœ€ç»ˆ{len(top_ideas)}ä¸ª")
    else:
        # å•é˜¶æ®µå¬å›ï¼ˆåŸé€»è¾‘ï¼‰
        similarities = []
        for idea in ideas:
            sim = compute_similarity(user_idea, idea['description'])
            if sim > 0:
                similarities.append((idea['idea_id'], sim))

        similarities.sort(key=lambda x: x[1], reverse=True)
        top_ideas = similarities[:TOP_K_IDEAS]

        print(f"  æ‰¾åˆ° {len(similarities)} ä¸ªç›¸ä¼¼Ideaï¼Œé€‰æ‹© Top-{TOP_K_IDEAS}")

    path1_scores = defaultdict(float)
    for idea_id, similarity in top_ideas:
        idea = idea_map[idea_id]
        # æ‰“å°åŒ¹é…åˆ°çš„ç›¸ä¼¼ Idea è¾…åŠ©è°ƒè¯•(å¢åŠ åˆ°300å­—ç¬¦)
        if similarity > 0.2:
            print(f"    - åŒ¹é… Idea [{idea_id}]: {idea['description'][:300]}... (sim={similarity:.3f})")

        # è·¯å¾„ 1 ç›´æ¥ä» Idea èŠ‚ç‚¹çš„ pattern_ids å¬å›
        pattern_ids = idea.get('pattern_ids', [])
        for pid in pattern_ids:
            path1_scores[pid] += similarity

    # æ’åºå¹¶åªä¿ç•™Top-Kä¸ªPattern
    sorted_path1 = sorted(path1_scores.items(), key=lambda x: x[1], reverse=True)
    path1_scores = dict(sorted_path1[:TOP_K_PATTERNS_PATH1])

    print(f"  âœ“ å¬å› {len(sorted_path1)} ä¸ªPatternï¼Œä¿ç•™Top-{TOP_K_PATTERNS_PATH1}\n")

    # ===================== è·¯å¾„2: é¢†åŸŸç›¸å…³å¬å› =====================
    print("ğŸŒ [è·¯å¾„2] é¢†åŸŸç›¸å…³æ€§å¬å›...")

    # é€šè¿‡æœ€ç›¸ä¼¼Ideaçš„Domain
    top_idea = idea_map[top_ideas[0][0]] if top_ideas else None
    domain_scores = []

    if top_idea and G.has_node(top_idea['idea_id']):
        for successor in G.successors(top_idea['idea_id']):
            edge_data = G[top_idea['idea_id']][successor]
            if edge_data.get('relation') == 'belongs_to':
                domain_id = successor
                weight = edge_data.get('weight', 0.5)
                domain_scores.append((domain_id, weight))

    domain_scores.sort(key=lambda x: x[1], reverse=True)
    top_domains = domain_scores[:TOP_K_DOMAINS]

    print(f"  æ‰¾åˆ° {len(domain_scores)} ä¸ªç›¸å…³Domainï¼Œé€‰æ‹© Top-{TOP_K_DOMAINS}")

    path2_scores = defaultdict(float)
    for domain_id, domain_weight in top_domains:
        # æ‰“å°Domainè¯¦ç»†ä¿¡æ¯
        domain = domain_map.get(domain_id)
        if domain:
            domain_name = domain.get('name', 'N/A')
            paper_count = domain.get('paper_count', 0)
            sub_domains = domain.get('sub_domains', [])
            sub_domain_str = ', '.join(sub_domains[:5])  # åªæ˜¾ç¤ºå‰5ä¸ªsub_domain
            if len(sub_domains) > 5:
                sub_domain_str += f"... (å…±{len(sub_domains)}ä¸ª)"

            print(f"  - {domain_id} (åç§°={domain_name}, ç›¸å…³åº¦={domain_weight:.3f}, è®ºæ–‡æ•°={paper_count})")
            if sub_domain_str:
                print(f"    å­é¢†åŸŸ: {sub_domain_str}")

        for predecessor in G.predecessors(domain_id):
            edge_data = G[predecessor][domain_id]
            if edge_data.get('relation') == 'works_well_in':
                pattern_id = predecessor
                effectiveness = edge_data.get('effectiveness', 0.0)
                confidence = edge_data.get('confidence', 0.0)
                path2_scores[pattern_id] += domain_weight * max(effectiveness, 0.1) * confidence

    # æ’åºå¹¶åªä¿ç•™Top-Kä¸ªPattern
    sorted_path2 = sorted(path2_scores.items(), key=lambda x: x[1], reverse=True)
    path2_scores = dict(sorted_path2[:TOP_K_PATTERNS_PATH2])

    print(f"  âœ“ å¬å› {len(sorted_path2)} ä¸ªPatternï¼Œä¿ç•™Top-{TOP_K_PATTERNS_PATH2}\n")

    # ===================== è·¯å¾„3: ç›¸ä¼¼Paperå¬å› =====================
    print("ğŸ“„ [è·¯å¾„3] ç›¸ä¼¼Paperå¬å›...")

    # ä¸¤é˜¶æ®µå¬å›ä¼˜åŒ–
    if TWO_STAGE_RECALL and USE_EMBEDDING:
        print(f"  [ç²—æ’] ä½¿ç”¨Jaccardå¿«é€Ÿç­›é€‰Top-{COARSE_RECALL_SIZE}...")
        coarse_similarities = []
        for paper in tqdm(papers, desc="Processing papers"):
            paper_title = paper.get('title', '')
            if not paper_title:
                continue

            sim = compute_jaccard_similarity(user_idea, paper_title)
            if sim > 0.05:  # é™ä½é˜ˆå€¼
                coarse_similarities.append((paper['paper_id'], sim))

        coarse_similarities.sort(key=lambda x: x[1], reverse=True)
        candidates = coarse_similarities[:COARSE_RECALL_SIZE]

        print(f"  [ç²¾æ’] ä½¿ç”¨Embeddingé‡æ’Top-{TOP_K_PAPERS}...")
        fine_similarities = []
        for paper_id, _ in candidates:
            paper = paper_map.get(paper_id)
            if not paper:
                continue
            paper_title = paper.get('title', '')

            sim = compute_embedding_similarity(user_idea, paper_title)
            if sim > 0.1 and G.has_node(paper_id):
                quality = get_paper_quality(paper)
                combined = sim * quality
                fine_similarities.append((paper_id, sim, quality, combined))

        fine_similarities.sort(key=lambda x: x[3], reverse=True)
        top_papers = fine_similarities[:TOP_K_PAPERS]

        print(f"  âœ“ ç²—æ’{len(coarse_similarities)}ä¸ª â†’ ç²¾æ’{len(candidates)}ä¸ª â†’ æœ€ç»ˆ{len(top_papers)}ä¸ª")
    else:
        # å•é˜¶æ®µå¬å›ï¼ˆåŸé€»è¾‘ï¼‰
        similarities = []
        for paper in tqdm(papers, desc="Processing papers"):
            paper_title = paper.get('title', '')
            if not paper_title:
                continue

            sim = compute_similarity(user_idea, paper_title)
            if sim > 0.1 and G.has_node(paper['paper_id']):
                quality = get_paper_quality(paper)
                combined = sim * quality
                similarities.append((paper['paper_id'], sim, quality, combined))

        similarities.sort(key=lambda x: x[3], reverse=True)
        top_papers = similarities[:TOP_K_PAPERS]

        print(f"  æ‰¾åˆ° {len(similarities)} ä¸ªç›¸ä¼¼Paperï¼Œé€‰æ‹© Top-{TOP_K_PAPERS}")

    path3_scores = defaultdict(float)
    for paper_id, similarity, quality, combined_weight in top_papers:
        paper = paper_map.get(paper_id, {})
        # åˆ¤æ–­è´¨é‡æ¥æºï¼šä¼˜å…ˆæ£€æŸ¥review_statsï¼Œç„¶åæ˜¯reviewsï¼Œå¦åˆ™æ˜¯é»˜è®¤å€¼
        if paper.get('review_stats'):
            quality_source = f"review({paper['review_stats'].get('review_count', 0)}æ¡)"
        elif paper.get('reviews'):
            quality_source = "review"
        else:
            quality_source = "é»˜è®¤"
        title = paper.get('title', 'N/A')
        print(f"  - {paper_id} (ç›¸ä¼¼åº¦={similarity:.3f}, è´¨é‡={quality:.3f} [{quality_source}])")
        print(f"    æ ‡é¢˜: {title}")

        if not G.has_node(paper_id):
            continue
        for successor in G.successors(paper_id):
            edge_data = G[paper_id][successor]
            if edge_data.get('relation') == 'uses_pattern':
                pattern_id = successor
                pattern_quality = edge_data.get('quality', 0.5)
                path3_scores[pattern_id] += combined_weight * pattern_quality

    # æ’åºå¹¶åªä¿ç•™Top-Kä¸ªPattern
    sorted_path3 = sorted(path3_scores.items(), key=lambda x: x[1], reverse=True)
    path3_scores = dict(sorted_path3[:TOP_K_PATTERNS_PATH3])

    print(f"  âœ“ å¬å› {len(sorted_path3)} ä¸ªPatternï¼Œä¿ç•™Top-{TOP_K_PATTERNS_PATH3}\n")

    # ===================== èåˆç»“æœ =====================
    print("ğŸ”— èåˆä¸‰è·¯å¬å›ç»“æœ...\n")

    all_patterns = set(path1_scores.keys()) | set(path2_scores.keys()) | set(path3_scores.keys())

    final_scores = {}
    for pattern_id in all_patterns:
        score1 = path1_scores.get(pattern_id, 0.0) * PATH1_WEIGHT
        score2 = path2_scores.get(pattern_id, 0.0) * PATH2_WEIGHT
        score3 = path3_scores.get(pattern_id, 0.0) * PATH3_WEIGHT
        final_scores[pattern_id] = score1 + score2 + score3

    ranked = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)
    top_k = ranked[:FINAL_TOP_K]

    # ===================== è¾“å‡ºç»“æœ =====================
    print("=" * 80)
    print(f"ğŸ“Š å¬å›ç»“æœ Top-{FINAL_TOP_K}")
    print("=" * 80)

    for rank, (pattern_id, final_score) in enumerate(top_k, 1):
        pattern_info = pattern_map.get(pattern_id, {})

        score1 = path1_scores.get(pattern_id, 0.0) * PATH1_WEIGHT
        score2 = path2_scores.get(pattern_id, 0.0) * PATH2_WEIGHT
        score3 = path3_scores.get(pattern_id, 0.0) * PATH3_WEIGHT

        print(f"\nã€Rank {rank}ã€‘ {pattern_id}")
        print(f"  åç§°: {pattern_info.get('name', 'N/A')}")
        print(f"  æœ€ç»ˆå¾—åˆ†: {final_score:.4f}")

        if final_score > 0:
            print(f"  - è·¯å¾„1 (ç›¸ä¼¼Idea):   {score1:.4f} (å æ¯” {score1/final_score*100:.1f}%)")
            print(f"  - è·¯å¾„2 (é¢†åŸŸç›¸å…³):   {score2:.4f} (å æ¯” {score2/final_score*100:.1f}%)")
            print(f"  - è·¯å¾„3 (ç›¸ä¼¼Paper):  {score3:.4f} (å æ¯” {score3/final_score*100:.1f}%)")

        print(f"  èšç±»å¤§å°: {pattern_info.get('size', 0)} ç¯‡è®ºæ–‡")

        # V3ç‰ˆæœ¬: ä¼˜å…ˆæ˜¾ç¤ºLLMå¢å¼ºçš„æ€»ç»“ï¼Œå¦åˆ™æ˜¾ç¤ºåŸå§‹ç¤ºä¾‹
        if pattern_info.get('llm_enhanced_summary'):
            llm_summary = pattern_info['llm_enhanced_summary'].get('representative_ideas', '')
            print(f"  å½’çº³æ€»ç»“: {llm_summary[:120]}...")
        else:
            summary = pattern_info.get('summary', {})
            ideas = summary.get('representative_ideas', [])
            if ideas:
                print(f"  ç¤ºä¾‹Idea: {ideas[0][:120] if ideas else 'N/A'}...")

    print("\n" + "=" * 80)
    print("âœ… å¬å›å®Œæˆ!")
    print("=" * 80)


if __name__ == '__main__':
    try:
        main()
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
